# Section 1 – Attacks, Threats, and Vulnerabilities

## 1.1 – Social Engineering

### Phishing 

Phishing is a type of social engineering attack where cybercriminals use various tactics, such as email, phone calls, or fake websites, to trick individuals into providing sensitive information. Here are some important points to consider:

1.  Types of Phishing: There are several types of phishing attacks, including spear phishing, clone phishing, whaling, and smishing. Spear phishing targets specific individuals, while clone phishing involves creating a fake website that looks identical to a legitimate one. Whaling targets high-level executives, while smishing uses SMS text messages to deceive individuals.
    
2.  Methods of Attack: Cybercriminals use various methods to carry out phishing attacks, such as email spoofing, which involves creating a fake email address that appears legitimate. They may also use link manipulation, which redirects individuals to a fake website or download malware onto their device.
    
3.  Warning Signs: Individuals should be aware of warning signs, such as suspicious email addresses, unexpected attachments, or requests for personal information. Fake websites may also have irregular URLs or spelling errors.
    
4.  Impact of Phishing: Phishing attacks can result in a range of negative consequences, such as financial loss, identity theft, or reputational damage. It can also lead to the compromise of sensitive business or personal data.
    
5.  Prevention: Individuals can prevent phishing attacks by being vigilant, verifying the sender's identity, and avoiding clicking on links or downloading attachments from suspicious sources. Employers can also provide employee training and implement security measures such as multi-factor authentication and spam filters.
    

Overall, phishing attacks are a significant threat to individuals and businesses alike. By understanding the methods used by cybercriminals and implementing preventative measures, individuals and organizations can better protect themselves from these types of attacks.

### Impersonation    

Impersonation is a type of cyberattack where an individual or entity pretends to be someone else in order to deceive others. Impersonation can take several forms, including:

1.  Email Impersonation: An attacker can impersonate someone by sending an email that appears to be from that person, typically with a fake email address or a slightly altered name.
    
2.  Social Media Impersonation: An attacker can create fake social media accounts that look like someone else's account, with the aim of deceiving the target's friends, family or followers.
    
3.  Caller ID Spoofing: Attackers can spoof the caller ID information to make it appear as if a call is coming from a legitimate source.
    
4.  Website Spoofing: Attackers can create fake websites that look like legitimate ones to trick people into sharing sensitive information.
    

The goal of impersonation is typically to obtain sensitive information, such as login credentials, financial information or personal data. Impersonation can also be used for other malicious purposes, such as spreading misinformation or launching a social engineering attack.

To prevent impersonation attacks, it is important to be vigilant and verify the identity of the sender or caller. This can be done by using two-factor authentication, checking the sender's email address or phone number, and being cautious when opening links or attachments from unknown sources. Additionally, organizations can implement security measures, such as email filters and training programs, to help prevent impersonation attacks.

### Dumpster Diving    

Dumpster diving is a type of physical security attack where an attacker searches through a targeted organization's trash or recycling bins in order to find valuable information. The attacker may use information obtained through dumpster diving to launch a spear phishing attack, impersonating a trusted source to gain access to sensitive information or financial resources. In many places, dumpster diving is generally legal unless there are local laws or regulations that prohibit it. To prevent dumpster diving attacks, it is important to properly dispose of sensitive information and implement policies and procedures for handling sensitive information.

### Shoulder Surfing    

Shoulder surfing is when someone gathers personal and sensitive information by looking over your shoulder while you work on your computer. This is a common way for attackers to obtain password information and details about documents that you might be viewing on your computer. It can be done from close range, such as in an airport or on a plane, or even from a distance using binoculars or a telescope. It's important to be aware of your surroundings and to use tools like privacy filters to protect your screen and prevent shoulder surfing. In an office, it's also helpful to position your monitor away from windows or hallways to avoid exposure.

### Hoaxes    

The hoaxes often aim to get money by making people purchase gift cards and send the codes to the hoaxers. Some hoaxes may even make people believe they have a virus or malware on their system. The author provides examples of recent hoaxes they encountered, including a spam email claiming the recipient won $850,000 and a fake Adobe Flash Player update page that could install malware. To avoid falling victim to hoaxes, it's important to be suspicious of unsolicited messages, double-check information, use spam filters, and cross-reference with websites such as hoaxslayer.net and snopes.com.

### Watering Hole Attacks    

A watering hole attack is a tactic used by attackers to compromise a target organization's network by infecting a third-party website that is frequently visited by the organization's users. The attackers focus on finding vulnerabilities in the third-party website and attempt to infect as many visitors as possible. The attack is difficult to defend against, but a layered defense strategy with a next-generation firewall or intrusion prevention system can help mitigate the risk. One example of a watering hole attack occurred in 2017 on multiple financial sites. Users who had Symantec antivirus software installed were able to detect and block the attack. It's important to be aware of these types of attacks and take measures to prevent them.

### Spam    

The term "spam" refers to unsolicited messages that are sent through various communication channels, such as email, online forums, instant messaging, and text messaging. Spam messages can be commercial, non-commercial, or malicious, and they pose various problems for system administrators, including security concerns, resource utilization, and costs associated with storing and managing spam messages.

There are several strategies for preventing spam from reaching users' inboxes, including having a spam filter that can filter messages based on various characteristics, such as an allowed list of trusted senders, compliance with email transfer standards, and reverse DNS lookup. Other techniques include tar pitting, which slows down the conversation between servers and frustrates spammers, and recipient filtering, which rejects messages that are not addressed to a valid email address.

### Influence Campaigns    

The rise of social media has allowed for individuals or nation-states to manipulate public opinion. Bad actors may create fake accounts to spread their message across multiple social media platforms, with the intention of amplifying their message and changing the way people think in a particular area. The success of such influence campaigns can lead to military advantages, such as influencing elections or changing the type of news received by a particular country. This is known as cyber warfare, and while it is not fought on a traditional battlefield, it can still have significant impacts on global relations.

### Other Social Engineering Attacks    

Social engineering attacks such as tailgating, invoice scams, and credential harvesting are all serious threats to organizations and individuals. Tailgating occurs when an unauthorized person follows someone with valid credentials into a restricted area, while an invoice scam involves sending a fake invoice to an organization in order to obtain payment or sensitive information. Credential harvesting is the process of stealing usernames and passwords from an individual's computer, often through the use of malicious scripts.

Organizations can take steps to prevent these types of attacks, such as implementing access control vestibules, reminding employees not to tailgate, and training staff to recognize and report suspicious emails and requests. Individuals can also protect themselves by being vigilant about who they allow access to restricted areas, verifying the authenticity of invoices and requests, and using antivirus and anti-malware software to protect their computers from malicious scripts. By taking proactive measures to prevent social engineering attacks, organizations and individuals can better protect themselves against these threats.

### Principles of Social Engineering    

Social engineering attacks are constantly evolving and becoming more sophisticated. Attackers use various tactics, such as authority, intimidation, social proof, scarcity, familiarity, and trust, to gain access to sensitive information or control of accounts. These attacks may involve multiple people or organizations, and can be automated using open source intelligence. A real-life example is the case of Naoki Hiroshima, whose Twitter username was stolen using social engineering techniques. It is important to be aware of these tactics and take steps to protect against social engineering attacks.

## 1.2 – Attack Types

### An Overview of Malware    

Malware is malicious software that can collect information from your computer, control it remotely, show unwanted advertisements, and even encrypt your private files for ransom. There are various types of malware, including viruses, ransomware, worms, Trojan horses, rootkits, key loggers, adware, spyware, and botnets. Malware can spread through vulnerabilities in your operating system or applications, or through deceptive email links and website pop-ups. To prevent malware from infecting your computer, it's important to maintain the latest security updates and never click suspicious links or download unknown software.

### Viruses and Worms    

A virus is a type of malware that requires human intervention to start the replication process. There are different types of viruses, such as program viruses, boot sector viruses, script viruses, macro viruses, and fileless viruses. A worm, on the other hand, is a type of virus that can propagate itself from system to system without human intervention by exploiting vulnerabilities in operating systems or applications. The WannaCry worm, which caused widespread damage in 2017, used the EternalBlue exploit to find vulnerable systems and then installed a backdoor application to download ransomware and encrypt files on the affected computer. It's important to keep antivirus software updated to protect against viruses and worms.

### Ransomware and Crypto-malware    

Ransomware is a type of malware that encrypts personal information on a computer or network and demands payment in exchange for the decryption key. This can include personal files, company documents, financial information, and more. Attackers use this method to make money by exploiting the value of the data to the owner. Simple ways to protect against ransomware include maintaining security patches, running up-to-date antivirus and anti-malware software, and having a backup that is not immediately accessible from the computer. This way, even if ransomware infects the system, the owner can recover the data without paying the attacker.

### Trojans and RATs    

A Trojan horse is a type of malware that pretends to be harmless software, but actually allows attackers to gain access to your computer and potentially install other malware. They can create backdoors into your system, making it easier for other malware to infect your computer. They may also install potentially unwanted programs (PUPs) which can cause performance issues. Remote Access Trojans (RATs) are a type of backdoor malware that allow attackers to remotely access and control your computer. To prevent Trojan horse and RAT infections, you should avoid clicking unknown links, keep your antivirus software up to date, and regularly backup your system.

### Rootkits    

Rootkits are a type of malware that modify files in the kernel of an operating system, making them difficult to detect and remove. They are commonly used in combination with other malware to create a botnet that is challenging to remove. However, anti-malware and antivirus software can identify and remove some rootkits. The UEFI BIOS secure boot feature helps prevent rootkits from being installed on modern systems by checking for changes to the kernel during bootup.

### Spyware    

Adware and spyware are types of malicious software that can cause performance problems, track your browsing habits, and potentially steal personal information. They can be installed accidentally and are often difficult to remove. Prevention measures include using good antivirus and anti-malware software, being cautious when installing software from third-party websites, and having a known good backup. Software such as Malwarebytes anti-malware can also be helpful in removing this type of malware.

### Bots and Botnets    

A bot is a type of malware that can take control of your computer and be used in a botnet to perform nefarious activities. The botnet is controlled by a Command and Control server, and can be used for DDoS attacks, spamming, and other tasks. To prevent a botnet infection, ensure your system is up to date with the latest security patches and use antivirus and anti-malware software. Active infections can be identified through on-demand scans and unusual network traffic patterns, and can be blocked with firewalls or IPS.

### Logic Bombs    

A logic bomb is a type of attack triggered by a separate event, often left by disgruntled employees or those with a grudge against an organization. It can be difficult to identify and may delete itself once executed. Examples of time-based logic bombs include one that deleted master boot records and rebooted systems in South Korea in 2013, and one that disabled electrical circuits and connections throughout Ukraine in 2016. To prevent logic bombs, formal processes and controls should be in place for any changes made in the environment, and automated processes such as host-based intrusion prevention or Tripwire can be used to look for certain changes. Auditing processes should also be in place to ensure all changes made are authorized.

### Password Attacks  

Passwords stored in plain text are easily accessible to attackers. Therefore, it is best to store passwords using a hash function, which produces a unique string of text from the password input. The resulting hash is unique to the password and cannot be reversed to reveal the original password. Password files in applications and operating systems typically store the usernames and the hash of each password. Attackers may use spraying attacks, which try a few common passwords to gain access to an account without triggering account lockouts, or brute force attacks, which try every combination of letters, numbers, and special characters until a matching password is found. Brute force attacks can take a long time and require performing the hash on each password to compare it to the original hash.

### Physical Attacks    

Various types of physical attacks, including the use of malicious USB cables and flash drives that can infect a system with malware. It also highlights the dangers of skimming, where attackers steal credit card information by adding hardware to card readers and using cameras to capture PIN numbers. The article emphasizes the importance of being cautious about connecting USB devices and carefully examining card readers to prevent skimming. It also mentions the practice of cloning credit cards and gift cards for fraudulent use.

### Adversarial Artificial Intelligence    

Machine learning has revolutionized the way we use computers by allowing them to sift through vast amounts of data and find patterns that can be used to provide us with services or information. However, a major challenge with machine learning is that it requires a lot of data to train the systems properly. If the training data is poisoned with malicious or invalid information, it can render the resulting artificial intelligence invalid as well.

Attackers can also change their approach after the learning process is complete, making it important to periodically retrain machine learning systems with new, legitimate data. It's also crucial to use techniques to prevent vulnerabilities in machine learning processes and ensure that the data going into the systems is legitimate. Examples of poisoned training data include the Microsoft AI chatter bot Tay, which became a racist, sexist, and inappropriate bot due to lack of anti-offensive behavior or algorithms during its learning process.

### Supply Chain Attacks    

The supply chain is the chain of manufacturing that gets a product from the beginning to the end of its process. There is a lot of opportunity for attackers to infect the supply chain and affect downstream users. The Target Corporation breach in 2013 is a notable example of a supply chain attack where the attackers were able to take advantage of an HVAC supplier to gain access to the Target network and steal 40 million credit card numbers. Supply chain cybersecurity has become a significant concern for organizations, and many are narrowing down the number of vendors they work with and requiring strict controls and auditing to ensure trust in the products delivered. Organizations can also work with their suppliers to ensure a safer product all the way through the supply chain.

### Cloud-based vs. On-Premises Attacks    

Both on-premises and cloud-based data storage have their own advantages and disadvantages when it comes to data security. On-premises data storage gives you complete control and allows you to customize your security controls, but it can be costly and may take time to implement security changes. Cloud-based storage is centralized and cost-effective, with large-scale security provided by the provider, but there is a reliance on third-party services and users need to follow best practices for security. Ultimately, the choice between on-premises and cloud-based storage will depend on the specific needs and resources of the organization.

### Cryptographic Attacks    

A man-in-the-middle attack is another type of attack that can compromise the security of cryptographic communication. In this attack, an attacker intercepts the communication between two parties and alters the information being exchanged. The attacker can also eavesdrop on the conversation and steal sensitive information.

To prevent this type of attack, it’s important to use strong encryption protocols, such as TLS (Transport Layer Security) and to ensure that both parties are using the latest version of the protocol. It’s also important to verify the identity of the other party, for example, by using digital certificates.

Overall, it’s important to remain vigilant and keep up-to-date with the latest security vulnerabilities and patches to ensure that our cryptographic communications remain secure.

## 1.3 – Application Attacks

### Privilege Escalation    

Privilege escalation is a serious threat that can allow attackers to gain elevated access to a system or resources that they would not normally have access to. It is important to patch vulnerabilities quickly and use safeguards like data execution prevention to prevent these types of attacks. Keeping up to date with security patches and using antivirus and anti-malware software can also help protect against privilege escalation attacks. It is also important to be aware of vulnerabilities like the Windows Remote Access Elevation of Privileged Vulnerability and take appropriate action to protect against them.

### Cross-site Scripting    

Cross-site scripting (XSS) is a vulnerability found on web-based applications that allows attackers to gather information from a victim's machine without their knowledge. There are two types of XSS attacks: non-persistent or reflected, and persistent or stored. Reflected attacks require the victim to click a specific link containing malicious scripts that are then sent to the attacker. Stored attacks can be permanently embedded on a server and run whenever anyone visits the page, potentially spreading very quickly over social media. A practical example of XSS was found in the Subaru website when a security researcher discovered that an authentication token allowing access to service requests never expired and could be used by anyone to gain access to the victim's account details. Additionally, the website had a cross-site scripting vulnerability that further compounded the problem. It is essential to prevent these types of scripting vulnerabilities in web-based applications to protect user information.

### Injection Attacks    

Code injection attacks can be very dangerous and can allow an attacker to gain access to sensitive information or even take control of a system. It’s important for developers to validate and sanitize all input to prevent these types of attacks. As a user, it’s important to be aware of the risks and only trust applications and websites that have a good reputation for security. Always be cautious about entering personal information into a form or application that you’re not familiar with. By taking these precautions, we can help prevent code injection attacks and keep our information secure.

### Buffer Overflows    

A buffer overflow attack occurs when one section of memory is able to overwrite a different section of memory, which can give an attacker control over the system or cause an application to behave in a certain way. It is a result of poor programming, and developers should perform bounds checking to prevent it. It is difficult to find and replicate, but can be very dangerous if successful. A visual example is given to explain how it works.

### Replay Attacks    

A hacker can gather network information by installing a network tap or using logical ways like ARP poisoning. They can replay attacks by capturing information and pretending to be the original user. Pass the hash is a simple replay attack that involves stealing the hash value associated with a password that is sent across the network during authentication. Developers can avoid this attack by ensuring that the client and the server communicate over an encrypted channel and by salting the hash. It is important to secure browser cookies as they may contain personalization details or session management information, which can be used by hackers for replay attacks. The session hijack or sidejack attack requires the attacker to first gain access to the session ID. To prevent this attack, it is important to use an encrypted protocol such as SSL or TLS when communicating across the network. The attacker can also modify their browser cookies to make them look identical to those that would come from the victim’s computer.

### Request Forgeries    

CSRF attacks take advantage of the trust a website has in a user’s browser to send requests to a server using the user’s credentials. The video explains how this can be done using a hyperlink in an email, for example, and how it can be prevented using anti-forgery techniques, such as a cryptographic token. The video also explains server-side request forgery (SSRF), where an attacker sends specially crafted packets to a web server to perform requests on their behalf. The video emphasizes the importance of closing vulnerabilities before someone is able to take advantage of them.

### Driver Manipulation    

Attackers are constantly looking for new ways to exploit vulnerabilities in our systems. They may use zero day attacks or target trusted drivers to gain access to sensitive information. Shims in the operating system, such as the Windows compatibility mode, can also be exploited by malware authors to bypass security features. Refactoring or metamorphic malware is another tactic that makes it difficult for antivirus software to detect and stop the malware. As a result, layered approaches to security are necessary to protect against these constantly evolving threats.

### SSL Stripping    

SSL stripping is a type of attack where an attacker sits in the middle of the communication between a client and a server and modifies the data flow, so that it can see all the data in the flow. The attacker can do this by using proxy server configuration, ARP spoofing, or a rogue Wi-Fi hotspot. The attack can be avoided by upgrading and maintaining software on both the client and server workstation. The evolution of SSL/TLS has seen various versions being deprecated due to vulnerabilities, with TLS 1.2 and 1.3 being the latest standards. During an SSL stripping attack, the attacker intercepts the request for an encrypted page and sends it in the clear to the website visitor, while setting up an encrypted channel between themselves and the web server. The attacker can view all the information passing through the communication, including usernames and passwords. To avoid this type of attack, web browsers and servers should be configured to only use HTTPS.

### Race Conditions    

A race condition occurs when multiple things happen simultaneously and developers haven't planned for it. Attackers can exploit this with time-of-check to time-of-use attacks. A simple example of a race condition is transferring money between two accounts, where multiple users perform the transfer at the same time, leading to incorrect balances. Race conditions can have disastrous consequences, such as in the Mars rover Spirit reboot loop, the Northeast Blackout of 2003, and the radiation therapy machine incidents in the 1980s. It's important for developers to consider every possible scenario and plan for it in their software.

### Other Application Attacks    

Memory vulnerabilities can give attackers control over a device or system. A memory leak is where an application fails to return allocated memory back to the system, causing it to crash. An attacker can cause a null pointer dereference, making an application point to nothing in memory and causing it to crash. An integer overflow is where a large number is placed in a smaller section of memory, causing extra space to overflow into other areas, which an attacker can manipulate. A directory traversal attack allows an attacker to access parts of a server's file system that they should not be able to. Error messages can reveal too much information about the underlying system, which an attacker can use to exploit the system. Input into an application must be evaluated to prevent malicious input that can cause denial of service or access to sensitive information. An API attack involves manipulating the application programming interface to gain additional access.

## 1.4 – Network Attacks

### Rogue Access Points and Evil Twins    

A rogue access point refers to an unauthorized access point added to a network. This could be someone, such as an end user or an employee, who purchases an inexpensive access point and connects it to the network without proper authorization. While this might not necessarily be a malicious act, it poses a significant security risk if someone gains access to the wireless access point and ultimately the corporate network.

Creating a rogue access point is becoming easier and more accessible. One can easily purchase an access point and plug it into an Ethernet network connection or turn on wireless sharing within their mobile device or laptop. This can effectively turn their device into a rogue access point, making it essential to periodically review wireless environments to ensure that only authorized access points are present.

To detect unauthorized access points, third-party devices can be used to understand the wireless spectrum and who may be using the frequencies in the environment. Additionally, tools like the Wi-Fi Pineapple can set up a rogue access point to test whether other people on the network use it.

Network access control mechanisms, such as 802.1x, are essential in preventing rogue access points. It requires everyone connecting to the network to provide a username, password, or other authentication before being allowed access to the network. Even if someone were to install a rogue access point and an outsider accessed it, they still wouldn't gain access to the network without proper authentication.

A more sinister type of rogue access point is a wireless evil twin. This access point is designed to look exactly like the existing access points on the network but is put there for malicious reasons. The attacker typically achieves this by using a similar SSID name, similar configuration settings, or putting the access point in an area where users are likely to connect to it.

Wireless evil twins are a more significant concern for open networks like public Wi-Fi hotspots. An attacker can maliciously install a wireless evil twin and have people connect to it, thinking they are connecting to the legitimate public Wi-Fi network. To secure wireless communications, it's essential to ensure all communication sent across the network is encrypted using HTTPS and, preferably, a VPN client. This way, all traffic, regardless of its destination, will be encrypted over the wireless network.

### Bluejacking and Bluesnarfing    

Bluejacking refers to the act of an attacker sending an unsolicited message via Bluetooth to a victim's mobile device, such as a phone or tablet. Unlike cellular frequencies from a mobile carrier or 802.11 networks, Bluejacking is specific to using the Bluetooth communication channel.

Since Bluetooth typically operates within a radius of about 10 meters, the attacker would need to be in close proximity to the victim's device in order to send these messages. Some Bluetooth implementations also allow the attacker to send other types of media, such as contact cards or videos, along with the Bluejacking message.

While Bluejacking is generally considered a low-priority security concern since it only involves sending a message to someone's device, users should still be trained on what to do if they receive unsolicited messages on their mobile devices.

A higher-priority security concern is Bluesnarfing, which occurs when an attacker gains access to data on a mobile device via the Bluetooth communication channel. With Bluesnarfing, attackers can potentially access contact lists, emails, calendar information, and other sensitive data stored on the device.

Although Bluesnarfing was a significant security concern when it was first discovered in 2003, it was quickly patched. However, older devices that use Bluetooth may still be susceptible to this type of attack, and it is important to ensure that proper security measures are in place to prevent unauthorized access to sensitive data.

### Wireless Disassociation Attacks    

Imagine you are using your wireless network and everything seems to be working just fine, until suddenly the network disappears. You are left without wireless connectivity and then, without warning, it reappears. This keeps happening repeatedly, and since you are on a wireless network, there is not much you can do to maintain the connection.

This is possibly a wireless deauthentication attack, also known as a denial of service attack. The attacker causes all the devices on the wireless network to suddenly lose communication with the access point. Your mobile device sends management frames to the access point to connect and disconnect from the wireless network. Unfortunately, the original 802.11 standard did not protect these management frames, meaning an attacker can send false management frames to the access point and cause problems with your communication.

To demonstrate this vulnerability, we have a packet capture showing the clear text frame details. An attacker can use a Linux device to send deauthentication frames to a specific BSS ID and station address, causing the device to disconnect from the Wi-Fi network. As long as the attacker continues to send these frames, the device will not be able to reconnect to the wireless network. This is a significant vulnerability, and IEEE has already made changes to the 802.11 specification to address this issue.

The update, 802.11w, protects some of the more important management frames like disassociation and deauthentication, but not all management frames are encrypted. For instance, frames such as beacons, probes, authentication, and association frames are still sent in clear text. However, the 802.11w update was rolled into the 802.11ac standard. Therefore, if you are using 802.11ac or a later version of 802.11, your access point already has this protection, and an attacker cannot use a disassociation or deauthentication attack to remove you from your wireless network.


### Wireless Jamming    

Radio frequency jamming, also known as RF jamming, is a technique used by attackers to disrupt wireless networks and create a denial of service situation. The main objective of this type of attack is to decrease the signal-to-noise ratio at the receiving device, which can be either the end station or the access point. The signal-to-noise ratio refers to the relationship between the good signal received by a device and all of the other types of wireless signals that the device receives.

As long as the good part of the wireless signal is received and understood above all the other noise that may be in that particular spectrum, communication can continue. But if the amount of noise overwhelms the good signal, the signal-to-noise ratio will be decreased, and the receiving device will not be able to communicate on that wireless network. Sometimes, the disruption of the signal is not intentional, such as when someone turns on a microwave oven and it sends interference that causes the signal not to be received by the end stations. However, in the case of an attack, someone may be maliciously sending additional noise on to the network to prevent someone else from receiving that wireless signal.

Attackers use various techniques to create noise and conflict on the wireless spectrum. One way is to send a constant random amount of information over the network to overwhelm the good signal. This might also be a constant amount of traffic sending legitimate frames as well, and simply using up all of the available bandwidth to do that. This type of wireless jamming might also be intermittent, with the attacker intermittently sending random data or intermittently sending legitimate frames to disrupt the normal flow of communication.

The attacker may also put a little bit of a spin on the jamming by only sending jamming signals when someone else tries to communicate on the network, effectively finding one individual device and limiting that device from communicating on the network. To disrupt devices that are on a local wireless network, the jamming device needs to be relatively close so that it can overwhelm the good signal. This means that an attacker would either physically need to be somewhere near that wireless network or they would have needed to install a device somewhere near that physical network.

Finding the source of this particular jamming signal could be challenging. Many times, a "fox hunt" is done, where a directional antenna and headphones are used to move around and find the source of the signal. Attenuating the signal or making it less strong can help to get a better reading, and eventually, the signal can be triangulated to determine where it may be coming from.

Finding and resolving these jamming issues can be challenging, but with the right equipment and techniques, it is possible to locate and remove them from the network. It is important to note that intentional RF jamming is illegal in many countries and can result in severe penalties.

### RFID and NFC Attacks    

RFID, or Radio-Frequency Identification, is a technology used to track and identify items or people. It is commonly found in access cards, assembly lines, and even in pets or animals. RFID tags vary in size, from as small as a grain of rice to the flatter version seen in ID cards. The tags use radar technology, with the signal sent to the tag powering it to transmit its ID back to the reader. Some RFID tags can support bidirectional communication, and there are also powered implementations that do not require energy transmitted to the tag to send information back.

As RFID is a wireless communication, it is susceptible to the same vulnerabilities as any wireless technology. Data interception between the RFID tag and reader is a significant concern, especially if the communication is not encrypted. If the RFID tag contains information that can be changed, someone could spoof the reader and modify the contents of the tag. Jamming frequencies associated with RFID communication could also create a denial-of-service situation, rendering the tag unreadable.

Near Field Communication (NFC) is another type of RFID technology commonly found in mobile devices. NFC allows users to pay for goods at checkout or pair their mobile device with Bluetooth devices by simply moving their device close to the device they want to pair with. NFC can also be used as an authentication factor to unlock doors.

The security concerns with NFC are similar to those of RFID. Interference of frequencies can cause denial-of-service situations, while unencrypted communication can be intercepted and modified by a third party. To prevent someone from stealing a mobile phone and using it for an NFC transaction, apps that rely on NFC must properly authenticate users.

In summary, RFID and NFC are technologies used for tracking and identifying items or people. While RFID is commonly used in access cards and assembly lines, NFC is found in mobile devices and allows for contactless payments and pairing with Bluetooth devices. Both technologies are vulnerable to data interception and denial-of-service attacks if not properly secured. Proper authentication measures must be in place to prevent unauthorized access to NFC-enabled devices.

### Randomizing Cryptography 

Cryptography is a technique used to ensure the security of data by transforming it into an unintelligible form, and one of the core elements of cryptography is randomization. If the resulting encrypted data looks similar to the original plain text, it can be vulnerable to reverse engineering, and attackers may determine the original text. For example, if we encrypt an image of a puppy without randomization, the encrypted data will look almost exactly like the original image. This is because there is no randomization to the key that was used in the encryption process.

To add randomization to the cryptographic process, we need to add a nonce, which is an arbitrary number that is used only once. A nonce can be a random value or something that is randomized enough so that an attacker cannot guess or easily replicate it. One way to use a cryptographic nonce is during the login process. A server sends a random nonce to the user, and the user combines it with their password hash and sends it back to the server. The server then evaluates the password hash and nonce to authenticate the user. This ensures that the password hash sent from the client to the server is different every time the user logs in, making it difficult for an attacker to use it in a replay attack.

Another type of nonce commonly used in encryption is an Initialization Vector (IV), which is a way to add randomization to the encryption scheme. If we add an IV to an encryption key that we're using repeatedly, it makes the overall encryption method stronger. An IV is used in encryption methods such as WEP encryption and some implementations of SSL.

We may also include a nonce, called a "salt," for the password hashes stored permanently in our database. A salt is a way to ensure that the passwords stored are randomized across all users on the system. By using a different salt for each user, we can have multiple users using the same password, but the stored hash value would be different for each user account. This makes it difficult for an attacker to determine that all users have similar passwords.

In conclusion, randomization is a crucial element in cryptography. Adding a nonce to the cryptographic process, such as during the login process or by using an Initialization Vector, can make the encryption method stronger. By including a nonce, such as a salt, for the password hashes stored permanently in our database, we can ensure that the passwords stored are randomized across all users on the system, making it difficult for an attacker to determine that all users have similar passwords.

### On-Path Attacks    

An on-path attack, also known as a man-in-the-middle attack, is a type of attack where an attacker intercepts and in some cases modifies data being sent interactively across a network. The attacker sits in the middle between two stations, and the original data stream is intercepted and then passed on to the destination. This allows the attacker to read everything going back and forth between the two devices, and it may also allow the attacker to modify the information as it's being transmitted.

One common type of on-path attack is ARP poisoning, which is an Address Resolution Protocol poisoning. ARP, as a protocol, does not have any security associated with it, and devices receive and modify ARP tables without any type of authentication or encryption. An attacker can send ARPs to any device on the local subnet, and those devices would interpret the ARPs as if they were coming from a legitimate source. In this attack, the attacker will need to be on the local network and will send an ARP message to the victim device unprompted. Because ARP doesn't have any security associated with it, those types of messages will be received and interpreted by the receiving device. The victim device receives the ARP message, changes the information in the cache, and going forward, anything that's sent to a specific IP address will be sent to the attacker's Mac address. Once that poisoning is complete on both sides, anything sent between the victim's machine and the router will be relayed through the attacker's device.

On-path browser attacks are another type of on-path attack, which involves malware running on the victim's machine. This type of attack is usually not a person that's handling the relay, but an automated process within the malware. With the on-path browser attack, the malware simply sits in the background and waits for the victim to log into a sensitive account, such as a bank account. Once the victim logs in, the authentication is now complete, and the malware can capture login credentials, keystrokes, and other sensitive information. Because the attacker is on the same computer as the victim, they can see all of the data in its raw, unencrypted form.

In conclusion, an on-path attack is a dangerous type of attack that can occur without anyone knowing that anyone is sitting in the middle of the conversation. ARP poisoning and on-path browser attacks are common types of on-path attacks that can cause significant damage if not detected and prevented. It's important to use secure protocols and encryption to prevent these attacks and to keep your devices up-to-date with the latest security patches to minimize the risk of attack.

### MAC Flooding and Cloning    

In computer networking, every network card has a unique Media Access Control (MAC) address, which is also known as the physical address of the card. The MAC address is 48 bits long, which is equivalent to 6 bytes, and is written in hexadecimal. The first three bytes of the MAC address are called the Organizationally Unique Identifier (OUI), which represents the manufacturer portion of the address. The last three bytes are the serial number, and the manufacturer will increment this for every network card they produce.

Switches on local area networks work at the MAC address level. The switch interprets the content of the frame and forwards or drops the traffic between different interfaces. A switch maintains a list of all of the MAC addresses on the local network, which helps it know where to send the frames as the destination. Another crucial operational requirement for a switch is that it maintains a loop-free environment, which it does by using the Spanning Tree Protocol (STP).

When a switch receives a frame, it checks the MAC address table to determine where to send that frame. If the MAC address is not listed in the table, it will add that MAC address and the output interface to the table. In cases where an attacker takes advantage of the MAC address table's limited capacity by sending traffic with different source MAC addresses to the switch, the switch will continue to add those thousands of new MAC addresses to its lookup table, which will eventually fill up the table. When the table reaches maximum capacity, the switch will no longer direct individual frames but will send every frame to every interface on the switch, turning the switch into a hub.

When this happens, the attacker can collect all of the traffic being transmitted on the network because some of the information is no longer directed to a particular device and instead is sent to all devices. Therefore, the MAC address table's limited capacity is an area where attackers can take advantage and potentially carry out malicious activities on the network. It is essential to keep network security protocols and update them frequently to prevent attackers from exploiting any vulnerabilities.


### DNS Attacks    

DNS poisoning is an attack in which an attacker redirects traffic to their own website by modifying the DNS server. DNS poisoning can be executed in several ways. One way is to modify the host file located on individual devices, which takes precedence over any DNS queries. Another way is for the attacker to sit in the middle of the conversation with an on-path attack and modify a query being sent to a client. The attacker can also modify the DNS information on the legitimate DNS server itself.

In a case study, an attacker was able to control domain information for six hours, redirecting anyone who accessed that DNS server and gathered that information to the hacker's website instead of the legitimate bank website. Attackers can also create a domain name that is close enough to seem legitimate, called URL hijack, which is used to redirect people to pages that show ads instead of taking them to the legitimate website. Attackers may also take advantage of misspellings, purchasing domain names that are similar to legitimate sites and hoping that someone mistypes the URL and visits their site.

Attackers can gain access to an account at a registrar by brute-forcing a password, using social engineering, gaining access to the email associated with the account or other methods to gain authentication and make changes to that domain information. Once a domain is hijacked, the attacker can get people to visit the site and gain personal information or login credentials. The difference in the URL can also be used by an attacker to perform malicious software installation. There are many ways for the attacker to gain access to domain information and use it to their advantage.

### Denial of Service    

A Denial of Service (DoS) attack is an act of causing a service or a network to become unavailable. This is typically accomplished by exploiting a vulnerability in software or a design flaw in the system. An attacker may use a DoS attack to direct traffic to their own website or to act as a smokescreen while stealing data. DoS attacks are not always complex and may include simple actions such as pulling the power switch on a building. Sometimes, DoS attacks are caused by errors made by individuals, such as plugging in the wrong cables that inadvertently create a network loop, which can cause the entire network to become unavailable.

Attackers often use botnets to perform Distributed Denial of Service (DDoS) attacks. This involves using many devices to attack a single service simultaneously to create bandwidth spikes, making the service unavailable. Attackers may use DDoS amplification to increase the amount of traffic being sent during an attack. This involves reflecting a particular type of protocol from one service onto the victim's machine to send more information, making the attack more substantial.

One way attackers can use DNS to amplify a DDoS attack is by finding DNS resolvers that are open and available to anyone, then querying this device with a spoofed IP address. A botnet command and control service then sends a message to the botnet to send a 28 byte DNS query to the open DNS resolvers. These devices then send that request in, and the DNS servers receive the query from a spoofed IP address. The IP address that's being spoofed is the web server, and the DNS servers send the response to the web server that was spoofed in the original query. The large amount of traffic can overwhelm the web server and make it unavailable.

To protect against DoS and DDoS attacks, it's important to keep operating systems and applications up to date with the latest versions, turn on Spanning Tree Protocol to avoid inadvertently creating network loops, and manage bandwidth over internet connections to prevent causing a DoS for everybody else. Additionally, using firewalls, intrusion detection systems, and other security technologies can help protect against DoS and DDoS attacks.

### Malicious Scripts

Automating tasks on your network or operating system can save you time and effort. With automation, tasks can be performed without human intervention, and any problems can be quickly identified and resolved. This is because automation occurs at the speed of the computer and can identify and resolve problems faster than a person can.

PowerShell is a command line for Windows that extends the functionality of the normal Windows command line. It can run functions at the command line, PowerShell specific scripts, or executables right from the PowerShell command prompt. Attackers can use PowerShell to administer systems, access Active Directory, or modify files in the file system.

Python is a more generalized scripting language that is used across many different operating systems, including Windows, Mac OS, and Linux. It is also commonly used in a cloud-based environment to orchestrate application instances. Attackers may use Python to hack cloud-based system servers, routers, switches, and other infrastructure devices.

In Unix and Linux, shell scripts are used for automation. These scripts can be customized with a variety of shells, including bash, bourne, korn, and c shells. Attackers may use shell scripts to perform functions in a Linux environment.

Macros are a type of script that is specific to certain applications. They are designed to automate certain functions within the application but can also be used for malicious purposes. In the Microsoft Office line of products, Visual Basic for Applications (VBA) provides extensive automation. VBA can interact inside of Microsoft Office and talk directly to the operating system, making it a target for attackers.

An example of a vulnerability that an attacker may exploit is CVE-2010-0815, where VBA does not properly search for ActiveX controls in a document. This vulnerability allows an attacker to run their own code on the device and install malware, backdoors, or other types of malicious software.

## 1.5 – Threat Actors and Vectors

### Threat Actors    
The definition of a threat actor is an entity responsible for an event that impacts the security of another entity. This is typically the entity you are trying to protect your network and data from. This is the bad guy.

They are persistent because once they get into your network, they are there until you get rid of them. A report from FireEye in 2018 showed that the average time for attackers to remain undetected in North and South America was 71 days. This goes up to 177 days in Europe, the Middle East, and Africa. And in the Asia-Pacific region, the attacker can be in the network without anyone noticing for up to 204 days.

Some of the most dangerous threat actors may already be inside your network, such as employees or contractors working for your organization.

A nation-state threat actor is typically a government, often responsible for national security. A good example of a nation-state threat actor is the team of the US and Israel, who used a worm to destroy around 1000 nuclear centrifuges in Iran.

A hacktivist is a threat actor who is both a hacker and an activist. This is a hacker who has a purpose or target in mind when carrying out these threats or attacks against a third party. This is often associated with a political or social message, but it does not necessarily have to be limited to these areas. This type of hacking usually has no financial gain.

A Script Kiddie is a threat actor who does not necessarily have to be a child, but may focus on running very simple scripts to gain access to someone's network. The script kiddie is usually an outsider trying to access internal resources. They want to brag about gaining access to someone's network or leaking some data.

Organized crime is definitely a threat actor that goes beyond computers. This is a group of professional criminals who are always motivated by financial gain.

The term hacker has a very broad definition, but it usually refers to a technology expert. This could be a good-natured expert or a malicious one. There are many ethical hackers who are held to look at a network to gain access, find weaknesses, and then help solve these weaknesses to make the network stronger.


### Attack Vectors  
The attack vector is the method a attacker will use to gain access to your computer or network. If an attacker has direct access to hardware running an operating system, there are many attack vectors they can use. If they have physical access, they will find a way to enter that operating system. In many operating systems, they can restart the system into a certain management mode, change the administrator password, and gain full access to the operating system. And while there are ways to prevent someone from gaining this type of access to a system, it can be very difficult to do if they have direct access to the equipment.

Another common direct access attack vector is to add a keylogger to the keyboard. Keyboards are typically directly connected to these servers, and administrators are typing in their usernames and passwords. Attackers will put a keylogger into the keyboard system, so they can cut the connection to the keyboard and expose everything that has been typed.

Another direct access vector is a vector where you can easily connect a flash drive or other type of portable media and copy all the files on that server to a piece of media that you can take with you. And of course, if someone has physical access to the computer, they can simply pull the power cord, pour water into the system, and create a denial of service.

In wireless networks, there are a number of attack vectors you need to be aware of. Of course, you need to ensure that your access point is secure. Additionally, you need to make sure that your network is not designed to allow malicious access points. A worse form of malicious access point is a rogue twin. A rogue twin is specially designed as a hacking tool and is made to mimic or look very similar to access points already present in your network.

You should also ensure that your access points and clients are using the latest technologies. For example, in 2017, a security vulnerability was found in many clients using WPA2, and they found a key reinstallation attack called KRACK that could provide access to WPA2 networks. This was a security vulnerability that was quickly resolved with the updating of most wireless clients. There are also some older encryption technologies like WEP and WPA that have significant security vulnerabilities, so you want to make sure that you are running WPA2 or newer on your wireless access points.

Attackers can send phishing links via email and collect personal information directly from end-users or add malicious software or other malware to the email and get people to launch this software from their email clients.

In your organization, most things have a complete supply chain. Each of these steps on the way is an attack vector, so it's important to know exactly where your technology comes from and to make it as secure as possible.

In the 2013 Target credit card breach, attackers took advantage of the supply chain to gain access to the Target network. These attackers used a third party to target Target. This was one of Target's vendors who later had access to the internal Target network. When the attacker gained access to the Target network, they effectively had access to all the cash registers at every Target location.

An example of using the supply chain to disrupt a production process is the Stuxnet worm in 2010. It was a joint effort between the US and Israel that introduced a worm into Iran's uranium enrichment program, disrupting the centrifuges used in the production process. In 2020, network administrators began to realize that Cisco switches were not behaving as expected. There were at least two models of non-Cisco switches. Details about where these switches came from and what the purpose of having these counterfeit switches could be are still somewhat unknown, but it certainly highlights how important it is to have a very secure supply chain.

Attackers can gather a lot of information from social media, and sometimes this information can be too much. Simply by following someone's timeline, they can determine where you are and when you may have visited a place, or they can see that you will be on vacation for two weeks and not near your home. These social media attack vectors can also be used to attack multi-factor authentication. 

They can look at your social media account and learn where or when you were born, or they can learn the name of your school mascot according to your Facebook profile. This information can then be used during a password reset that would allow an attacker to gain access to your account. 

One way to access normally inaccessible data is to find ways to bypass existing security technologies. One way to do this is to connect a system that will allow you to transfer data from an organization once you are inside. This can be very easy with a USB drive, and attackers constantly use USB connections to collect information and bypass existing security controls. 

Cloud-based applications have brought a completely new set of attack vectors that need to be considered. Often these applications are public, so it is important not only that the application is secure, but also that the entire configuration of that application is secure.

### Threat Intelligence  

As a security expert, you may spend a significant amount of time researching potential threats that could be dangerous for your organization. These threats can come from general or specific threat databases, information obtained directly from computer hackers, or information that you collect from other sources on the internet.

The important thing is to be aware that a threat exists, and therefore, it is essential to stay up-to-date on the latest threat postings and fully understand which threats could be relevant to your organization. Threat intelligence reports are available to almost everyone involved in IT security. Therefore, it is crucial to know exactly where to go to obtain this information.

A good place to start collecting this intelligence is through open sources. This is called OSINT, or Open Source Intelligence. This can come directly from the internet, discussion groups, or social media sites, or it can come from a government agency that gathers information from meetings or reports and makes it available to the public.

There is also intelligence that you can collect from commercial sources. That is, financial information, database maps, and other public information. As you can imagine, this threat intelligence can be very valuable, and some organizations are tasked with gathering this information and making it available to you for a fee.

This threat intelligence is presented in a way that allows you to easily see which threats could affect your organization. Additionally, it allows you to automate some workflows that can be automatically identified when a specific threat emerges.

A common threat intelligence source is vulnerability databases. These are large databases that compile information from many different researchers. Researchers will find a vulnerability, report it to the Vulnerability Database, and then the database will publish it to everyone.

A popular database is the Common Vulnerabilities and Exposures database or CVE. This is supported by the US Department of Homeland Security and the Cybersecurity and Infrastructure Security Agency. All of this information is compiled in a database that you can find online at the US National Vulnerability Database or NVD.

As you can imagine, sharing this vulnerability information can be valuable for everyone. One of the biggest challenges with all of this data is ensuring that you can get the data quickly and that the information you receive is of the highest quality. To help work towards these goals, the Cyber Threat Intelligence or CTA was created where members upload information they have about a specific threat, which is then evaluated and presented to other members of the organization.

As you can imagine, there is a lot of threat information that needs to be disseminated and this information needs to be transferred securely. This is why the industry has created Automated Indicator Sharing or AIS. This is a way to automate this process and move this information between organizations at internet speeds.

To be able to transfer this data, there needs to be a standardized format for these threats and the standardized format is called STIX. This is a Structured Threat Information eXpression that includes information such as motivations, capabilities, and response information.

To exchange this information securely, you need a type of trusted transport and that trusted transport is called TAXII. This is Trusted Automated eXchange of Indicator Information. We use this standardized TAXII format to transfer STIX data between organizations.

A unique and significant type of threat intelligence comes from the dark web. This is a layer on top of the existing internet that requires special software to access these specific websites. There is comprehensive information to be gathered from the dark web, including hacker group activities.

There are a number of communication channels available on the dark web and these forums can also be a valuable tool for searching for intelligence against attackers.

This is only a subset of indicators that can potentially identify a compromised network's security. It is important to deploy all of these indicators and more to be able to understand when and where someone has attacked your network.

### Threat Research    

If you are interested in knowing the threats associated with an operating system or an application, you should start with the companies that develop them. These vendors know their products better than anyone else, and they are usually the first to learn about any security vulnerabilities. Typically, their website includes a page where they track all known security vulnerabilities, and there is usually a notification process in place to immediately inform you when a new vulnerability is discovered.

The National Institute of Standards and Technology maintains a comprehensive database of security vulnerabilities. This is the National Vulnerability Database, which lists CVEs or Common Vulnerabilities and Exposures. It is common to supplement this database with third-party feeds obtained from other organizations.

Another good source of information comes from conferences. These are places where you can go to learn about the latest security vulnerabilities and threats that may affect you. Researchers who can provide information on new discoveries, trends in the industry, or the latest hacks often present their findings at these conferences.

If you want detailed information about the types of attacks and how people cope with them, you may want to consult academic journals. These are typically periodicals or online resources written by industry experts.

RFCs are a way of following and formalizing a set of standards that anyone on the internet can use. While many of these RFCs are standard documents, you will also find other types of documents, such as Experimental, Best Current Practice, Standard Track, and Historical Documents. Some of these RFCs also provide a detailed analysis of specific types of threats.

You can also find valuable information in user groups that are not specific to IT security, but rather technology organizations such as Cisco user groups, Microsoft user groups, or others. In these user groups, you not only learn valuable technical information, but you also meet local people whom you can use as a resource when you need them. There is a lot of security information on social media as well. The search feature on Twitter can also be a valuable tool for collecting information. You can search for a specific CVE, or search for terms like bug bounty or 0-day.

While conducting temporary social media searches can be useful, it's also important to be immediately aware when a threat emerges. Therefore, you should have access to an automated threat feed that will provide you with information on the most important threats you need to know about. There are many different sources for these threat feeds, such as the US Department of Homeland Security, the FBI, the SANS Internet Storm Center, VirusTotal Intelligence, and other publications.

## 1.6 – Vulnerabilities

### Vulnerability Types  

There are many ways for attackers to find their way inside of your network. The applications we use on our computers and workstations every day have vulnerabilities inside of them. Some of these are common vulnerability types.

If attackers use a vulnerability against you that has never been seen before, then we have a zero-day attack. A zero-day attack means we’ve never seen this vulnerability before, and there’s probably no patch or way to prevent this vulnerability from being exploited. You always want to keep an eye on the latest vulnerabilities, and one place to go is the Common Vulnerabilities and Exposures database, the CVE, located at [https://cve.mitre.org/](https://cve.mitre.org/).

Sometimes, attackers don’t need to find a hidden vulnerability inside software. Instead, they wait for you to leave the door open, and they simply walk in. This is an open permissions problem, where information has been put on the internet without any security applied to it. This makes it easy for anyone on the internet to access that information.

For example, in June 2017, Verizon accidentally exposed 14 million records of data in an Amazon S3 data repository. Instead of applying the proper passwords and security to the repository, it was left open.

Not only do we sometimes leave our data open, but we also sometimes leave our accounts open. If this account is an administrator account or root account, then an attacker may have full control over an operating system. Sometimes, this is a misconfiguration that allows someone access to an administrator account, or the password associated with the administrator account is not strong enough to prevent a brute force attack.

On many systems, the administrator has chosen not to allow interactive access to log into the administrator account. This means that no matter how hard the attacker tries to find the correct username and password combinations, they’ll never gain access to the operating system by logging in with the administrator or root account. We should always have policies and procedures in place to prevent casual use of these accounts.

For example, in December of 2015, the website Patreon had installed a debugger to help monitor a problem they were having with their website. Normally, this is something that would not be visible to the public and would only be used for internal use. Unfortunately, they left it turned on and it was exposed to the internet side.

Encrypting data doesn’t necessarily mean that it’s well-protected. There are many different kinds of encryption, so you need to be sure that you’re using strong encryption protocols. You also want to be sure that the hashes you’re using don’t have any known vulnerabilities. If you’re communicating over a wireless network, make sure you’re using the latest wireless encryption protocols. There are many different cipher suites, and you’re using different types of ciphers in different places on your network.

A good example of a technology where it’s important to stay up to date is the TLS protocol. This is the transport layer security protocol that we commonly use in our browsers to encrypt data. But there are over 300 cipher suites in TLS. Some of them are very secure, and some of them are not secure at all. So it’s important that you configure your web servers and your clients to make sure that they are using the strongest protocols. And with some of our applications, we’re simply sending the application data in the clear. Anyone who’s watching this data go back and forth on the network would be able to read everything that we’re sending back and forth. Protocols such as Telnet, FTP, SMTP, and IMAP are good examples of these in-the-clear protocols. In many cases, you can reconfigure the application to use an encrypted protocol instead of the in-the-clear protocol. So you might want to change the application to use SSH, SFTP, or IMAPS.

We’re connecting more and more devices to our networks these days, and most of these devices have a default username and password. Attackers know that many people will plug these in and never change that username and password. And they have found ways to use this to their advantage. One such use is the Mirai botnet. It takes advantage of these default usernames and passwords to gain access to these systems and take them over for their use. These devices now become part of a much larger botnet and are now under the control of the botnet owner. This is a botnet that takes advantage of many different kinds of devices, including cameras, routers, doorbells, garage door openers, and many other IoT, or internet of things, devices.

To be able to use these services over a network, we have to open ports on the server so that our applications can talk to the server itself. Unfortunately, opening these ports also creates an opening into the server, and we have to make sure that we’re properly adding the security that we need to let the good people in and keep the bad guys out. We often manage this flow of traffic with firewalls. We’ll have software-based firewalls running on the server, and we’ll have network-based firewalls on the ingress and egress parts of the network. The firewall will commonly have a rule set that will allow or disallow access to certain ports on the IP address and thereby keep out anyone who may be trying to attack that device.

Unfortunately, these rule sets tend to become very large and complex. And as time goes on, they become even more unwieldy. It may become very easy to accidentally allow access to a service that was not intended. In fact, it’s very common for someone who is managing one of these firewalls to occasionally audit the rule base, make sure that all of the rules are up to date, and that no mistakes have been made with IP addresses, port numbers, or any of the other services that are configured in that rule base.

We know that these vulnerabilities exist in our software and, of course, many organizations will occasionally release updates to the software that need to be deployed on all of our systems. Many organizations will have processes in place to be able to keep all of their systems up to date with the latest patches. This is a priority for many organizations because most of these patches are associated with security vulnerabilities.

There's usually a group of people that will test these patches, make sure that they will operate properly in your environment, and then load them on a central server, which will then deploy to all of the other systems in your organization. These patches may be associated with the firmware or the BIOS of the device. If the patches on your systems are not kept up to date, then the results could be very damaging.

Example: In 2017, between May and July, Equifax had a data breach of 147.9 million Americans, 15 million British citizens, and others. The information that was released included names, social security numbers, birth dates, addresses, and more. The attackers were able to get into these systems because a server on the Equifax network had not been properly patched.

If you go into any data center of really any size, you're going to find systems and components in there that may have been sitting there for a very long time. We refer to these older systems as legacy systems. These legacy devices may be running older operating systems, old applications. There may be middleware that's installed, but these are systems that we can't easily turn off or convert because perhaps they're performing a particular function that can't be duplicated or no one has really put in the effort to upgrade it to the latest version.

In many cases, these systems are running software that has been far beyond the end of life. There are no longer patches being released for these, and it now becomes a security concern. It's, of course, up to the security administrator to determine the advantage and disadvantage of having this system on the network and finding ways to protect it, even though there may be no way to patch the operating system.

### Third-party Risks    

No matter the size of your organization, there will be some type of third party that has access to your systems, applications, or data. And because these third parties exist, does not mean that we can have less security. We need just as much security because these third parties are on our network. 

It may be that the third parties are people that you can trust. But you should always plan for the worst possible scenario and make sure that your security policies and procedures are expecting those types of problems. And of course, these issues may not be malicious. It may just be errors that are created because everyone is human, and occasionally, problems will happen. You need to make sure that the security you’re putting in place for the technology and the physical security that you’re installing is taking into account all of these third parties. 

It may be that the third party is handling your hosting services, or maybe you contract with a third party to be able to do development work. In most of these cases, the system integrators have additional access to the systems because they need that access to be able to do their jobs. Even if the systems integrators are not on site, they still have access to the data. They may have virtual access to the data or through a terminal screen, or they may be physically on site and be able to install equipment, such as keyloggers or USB flash drives. And because these integrators are on the inside of the network, they’re past the firewalls and the security devices that we commonly put on the perimeter. That means they might be able to run software such as port scanners or capture data directly from the network without needing to go through any type of security controls. And if you’re on the inside, it’s much easier to put malware into an existing network, because you’ve now gone past all of those security filters. And in some cases, running software that you thought was safe may inadvertently install malware on systems. And now that those integrators are on the inside, it becomes much easier to deploy those instead of having to go through an existing email filter or firewall. 

We rely a lot on our vendors to be able to maintain the security of the systems that we’re putting into our environment. And very often, we have to make sure that the vendors know a problem exists and that they can fix the problem in a timely manner. This isn’t always the case. You have to, of course, make sure that the vendor is aware of the problem, and then the vendor themselves has to be motivated enough to make sure that they can keep those systems up to date and safe. 

For example, we can look at the situation that occurred with Trane Comfortlink II thermostats. These are thermostats that can be remotely managed and maintained. Trane was notified in April of 2014 that there were three security vulnerabilities associated with these thermostats, but it took a long time to have Trane finally resolve these particular vulnerabilities. Two of these were patched in April of 2015, a year later, and another one in January 2016, almost two years after these vulnerabilities were identified. 

These are the types of security issues that we rely on our vendors to resolve. We can't make these changes ourselves. Therefore, you must ensure that you partner with vendors who will be aware of these problems and able to react to them quickly.

For example, it's rare, but not unheard of, to bring software into the organization that may have previously been infected with malware. Even though you trusted the software coming from this third party, it was able to infect your systems once you installed the trusted software.

These days, you also have to check the hardware that you're getting from a third party. Some people have purchased Cisco switches, but what arrived, although it looked like a Cisco switch, was a counterfeit switch. Organizations need to have processes and procedures in place to monitor all of this coming through the supply chain and react to any security concerns.

For example, you have to decide where the code itself will be stored. If you have the code in-house, you may want to provide the developers with a VPN connection to all of that data or store the data on a centralized cloud-based server. In both situations, you need to make sure that you're putting in the correct security controls for where the data is and how people are accessing it.

It's also a good practice to ensure that wherever the data is stored and where the developers may be working is isolated and secure from the rest of the network. The production services should be on a separate, isolated part of the network, and the development team should not have access to the production site of the network.

With cloud-based services, we store a lot of information in a separate, third-party location. Some of this data needs to be evaluated for security as it may contain customer information, healthcare data, or financial details. We need to apply proper security around the type of data we're storing.

For example, there may be a mandate that healthcare information or financial information is stored in encrypted form, especially when stored at a third party. This protects the data against third-party access, but it also increases the complexities around managing the encryption process. If we're storing that data at a third-party location, we need to ensure that the transfer of data in and out of that facility is all done over an encrypted channel.

### Vulnerability Impacts    

One result of vulnerabilities may be the loss of data, and in some cases losing the data may be more damaging than losing money. For example, a database that has no password or is using default passwords can be at risk for losing the data within that database. An example of this data loss started in July 2020 with what the internet is calling the meow attack. This is databases that had no password or were using the default password, and all of the information in this database was being deleted without any type of warning. Researchers who were tracking this attack say that thousands of databases have been deleted, and instead of the data being in the database, all of that information has been replaced with the word "meow." This is an extreme example of what can happen if databases are not properly secured, and another reason why it’s very important to always have a backup. Some attackers don’t delete the data, but instead prefer to steal the data and then use that data for their own purposes.

A good example of this is the identity theft that occurred between May and July of 2017 at Equifax. The vulnerability that allowed this particular identity theft was a vulnerability in Apache Struts that was announced on March 7th. Attackers took advantage of a system that was not patched, on March 12th, and got into the Equifax network, and were able to remove all of this data from their systems.

An example of one of these reputation impacts occurred in October of 2016 with the company Uber. This particular breach allowed attackers to gain access to 25.6 million customer names, email addresses, and mobile phone numbers.

## 1.7 – Security Assessments

### Threat Hunting    

Attackers are constantly trying to find a way into your network to access your data. One of the problems we face in responding to these attacks is that we can't respond until the attack actually happens. One of the challenges we face in trying to identify these types of attacks is that there is a large amount of data that we need to examine just to determine if an attack has occurred. Therefore, the key is to take all of this data and put it into a large database, and then use big data analytics. The process of collecting data, identifying potential threats, and deploying systems that can protect our network against these attacks is ongoing.

### Vulnerability Scans  

If you work in IT security, you'll undoubtedly be doing some vulnerability scanning. These scans are designed to look at systems to see if there are any potential security vulnerabilities in an operating system, network device, or application. These are a little different from a penetration test that actually tries to gain access to the inner workings of your devices. Instead, vulnerability scanning is trying to determine from the outside if there is potential access to these systems.

A common type of vulnerability scan is a port scan. Here, we'll look at a device and determine which ports are responding to that particular IP address. From there, you can gather information about things that may not be secure. Vulnerability scans on all devices connected to the network are common. This can be servers, workstations, laptops, and other devices.

The software used by vulnerability scanners is a very powerful software designed to look at many different aspects of how your systems work in the hope of finding some security vulnerabilities on that device. We call these non-intrusive scans, but of course, there is a little intervention when scanning different port numbers and trying to find if there is a potential vulnerability.

However, these are not penetration tests. These vulnerability scanners do not try to exploit the vulnerability. Instead, they will decide whether a vulnerability exists or not. After the scan is complete, you can perform your own tests to see if this vulnerability really exists. You can run your own penetration test or find a specific exploit that can attack this vulnerability to see if it exists.

There are different approaches to perform these scans. One approach is to scan as if you were someone without network access. This will be an anonymous scan. This user does not have the credentials to log in to a device and obtain additional rights and permissions.

Let's go back to the list of vulnerabilities. Here, you can see that there are other vulnerabilities besides mixed vulnerabilities, medium, low, and many information security vulnerabilities. Now you need to decide which of these vulnerabilities are important, which ones you need to prioritize, and which ones may not affect you or have an impact on you in the list.

In addition, there are many online resources that can provide you with the information you need to make decisions when these security vulnerabilities are found. One widely used resource is the Unified CVE database in the National Vulnerability Database. You can find it at nvd.nist.gov. This is a summary of all CVEs that you can also find in the general security vulnerabilities and risks database, and you can find it at cve.mitre.org. You may also want to go directly to the manufacturers themselves. A great place to get information about Microsoft Windows is directly from Microsoft. You can find these Microsoft security bulletins at [www.microsoft.com/technet/security/current.aspx](http://www.microsoft.com/technet/security/current.aspx). However, another feature found in the National Vulnerability Database is the Common Vulnerability Scoring System. This provides a number associated with the vulnerability and can give you an idea of how serious the vulnerability might be.

There are currently two different scoring methods used, one is scoring version 2.0 and the other is currently scoring version 3.1. They use different criteria to generate the score, so make sure you choose the version you want to follow and then compare it to all the security vulnerabilities you find.

One of the challenges of security vulnerability scans is that sometimes you may find a security vulnerability that is occasionally reported, go and investigate the vulnerability, and find that the vulnerability scan is incorrect. In fact, this security vulnerability does not exist on the device. We call these false positives because our security vulnerability scan positively identified this vulnerability. However, after researching it, we realize that the positive indicator was actually wrong. And the false positive can now be ignored and we can continue our research.

### Security Information and Event Management    

To manage the large amount of information you receive from daily files and event notifications, you will want to use a Security Information and Event Management (SIEM) device. SIEM is designed to gather information from anything on the network that can generate log files, security alerts, or any type of real-time information that can tell us what is currently happening on the network. SIEM is typically used as a central repository, so everything will be recorded and collected in the central database of the SIEM. From there, you can create reports and historical perspectives over time about what has been happening on the network.

The data fed to SIEM comes from very different devices. A Windows server works very differently from a router or switch, which works very differently from a Linux workstation. Therefore, there must be a standard way to send log files from other devices to this central repository. And that standard is called syslog. There is usually a log collector, which is part of the SIEM, that is compatible with the system logs. This collector is waiting for messages to be sent from all these different devices on the network. The format of the messages received by the log collector is also in this standard system log format.

There is a continuous flow of traffic and information to SIEM. The logs added to SIEM can be parsed and Sim can then determine whether there is a security exception in these log files. Another type of analysis looks at how people behave. This way, you have user and entity behavior analytics that examine how people use the network. Another type of analytics is sensitivity analytics, which examines how the public perceives a particular organization. For example, if an organization has a very bad public image, it tends to attract hackers who could cause problems on that network.

Today, organizations are trying to take advantage of all this data and technology by using SOAR. SOAR stands for Security Orchestration Automation and Response. The goal of SOAR is to secure and automate these manual or tedious processes, making it possible to do all of this at computer speed.

## 1.8 – Penetration Testing

### Penetration Testing    

Cyber attackers are constantly trying to penetrate networks and gain access to sensitive data. The methods and tools they use are constantly evolving, and the attacks come from many different sources simultaneously. This makes it challenging for organizations to detect and respond to these threats effectively.

One of the biggest challenges is dealing with the vast amount of data that needs to be analyzed to identify potential attacks. There are many different sources of data, including logs, network traffic, and threat intelligence feeds. This information needs to be analyzed using big data analytics to detect patterns and anomalies that may indicate an attack.

Organizations must also be prepared to respond quickly and effectively to cyber attacks. This requires a comprehensive incident response plan that outlines the steps that should be taken in the event of an attack. It is also important to have the right security technologies in place to protect against these threats, including firewalls, intrusion detection and prevention systems, and security information and event management (SIEM) tools.

Another key component of a strong cyber defense strategy is employee training and awareness. Employees should be trained on how to recognize and avoid common cyber threats, such as phishing attacks and malware infections. They should also be educated on the importance of following security policies and procedures, such as strong password management and regular software updates.

Ultimately, the goal of any cyber defense strategy is to minimize the risk of a successful attack and minimize the impact if an attack does occur. This requires a proactive approach that combines technology, people, and processes to detect, prevent, and respond to cyber threats effectively. By staying up to date with the latest threats and best practices, organizations can stay one step ahead of cyber criminals and keep their data safe and secure.

### Reconnaissance    

Cyber attackers are constantly seeking ways to infiltrate networks and gain access to sensitive data. This is a continuous process carried out by multiple attackers from different locations and aimed at various systems simultaneously. The strategies employed to combat these attacks today may not be effective tomorrow as the attackers are constantly modifying their approach based on the defenders' reactions. One major challenge in preventing these attacks is the huge amount of data that needs to be sifted through to identify them.

Data sources vary, and different teams in the organization are responsible for monitoring different types of data. Correlating and identifying individual important pieces of data is critical to detecting attacks. Big data analytics is a mathematical process that can help with this task. This allows predictive analysis to anticipate potential problems and enable the deployment of security technologies to mitigate them.

Deployment of security technologies can be virtualized, and with big data analytics, threats can be detected and prevented from multiple sources simultaneously. This process involves constant data collection, threat identification, and deployment of protective systems. Threat feeds from third parties, governmental agencies, and social media can also provide valuable information in detecting potential threats. The use of big data analytics and deployment of security technologies can help organizations to stay ahead of cyber attackers and protect their networks and data.

### Security Teams    
The IT security field involves a variety of tasks and responsibilities, with different teams focusing on different aspects of security. The red team is responsible for offensive security measures, such as performing penetration tests and identifying vulnerabilities in systems. The blue team focuses on defensive security measures, including day-to-day operational security and incident response. In some organizations, these teams may be combined into a purple team to facilitate better communication and collaboration.

The white team oversees both the red and blue teams and acts as a manager or referee of sorts, enforcing rules and resolving any issues that arise. They may also be responsible for scoring the performance of the red and blue teams and compiling the results of penetration tests. By working together and sharing information, these teams can ensure the safety and security of an organization's systems and data.


# Section 2 – Architecture and Design

## 2.1 – Enterprise Security

### Configuration Management    

Managing information technology (IT) configurations can be challenging due to constant changes, such as new operating systems, patches, application upgrades, and network modifications. The IT team's primary goal is to document these updates and changes to track the configurations of all systems. Good documentation allows rebuilding an entire application instance by referring only to the documentation.

Network diagrams play a crucial role in documenting IT configurations, including physical device locations and connectivity. A diagram showing device connections can help track and manage patch cables and patch panel locations, making it easy to identify and track the wire's path from beginning to end.

In a data center, equipment is often spread across different racks, making it challenging to manage without proper documentation. Documentation of each rack's physical and internal components can help identify equipment and their locations. Similarly, documenting an application's firewall settings, patch levels, and operating system version can help create a baseline to track any changes.

To ensure documentation accuracy, integrity measurement checks against established baselines are necessary. Standardized naming and numbering formats for devices and cables can help track equipment location and changes effectively. Standardizing network interface and distribution panels and IP addressing can also help manage and track IT configurations.

Overall, extensive documentation is critical in managing IT configurations, making it easy to track and manage equipment and configurations.

### Protecting Data  

An organization's data is a vital asset, and as an IT security professional, it's your responsibility to ensure that data is safe. However, protecting data can be a challenge, especially considering that data is located in so many different places. It can be on a storage drive, traversing the network, or stored in the memory or CPU of a system.

One effective way to protect data is through encryption. Encryption transforms plain text into ciphertext, a scrambled version of the original data. A key is needed to unlock the ciphertext and turn it back into readable plain text. Encryption ensures that if a data breach were to occur, the stolen data would be unreadable without the proper key.

Another way to protect data is by implementing specific security policies, including different permissions for employees depending on their roles in the organization. This ensures that only authorized personnel can access the data. Data masking is another technique that can be used to protect sensitive data, such as personal identification information (PII). Masking replaces sensitive data with asterisks or other characters to obscure it and make it harder to read.

It's important to note that data sovereignty plays a significant role in protecting data. Data sovereignty refers to the laws and regulations surrounding the protection of data depending on where it's geographically stored. For example, the General Data Protection Regulation (GDPR) is a set of regulations in the European Union that requires data collected on EU citizens to be stored in the European Union. Other countries have similar rules, so it's essential to understand these laws and regulations if you're collecting and storing information.

Data protection also involves considering data in transit, which refers to data moving across a network. Encryption is one way to protect data in transit. When data is in transit, it's vulnerable to interception by hackers. By encrypting data in transit, even if it's intercepted, the encrypted data would be unreadable without the proper key.

Data at rest refers to data that is stored on a storage device. To protect data at rest, encryption can also be implemented. Whole disk encryption, database encryption, or individually encrypting files or folders are all effective ways to protect data at rest. It's also important to assign permissions to data stored on a storage device, so only authorized personnel can access it.

An organization's data is valuable, and protecting it is critical to the success of the business. IT security professionals must remain vigilant in protecting data by implementing effective security policies, using encryption, and understanding data sovereignty. By implementing these strategies, an organization can protect its data from unauthorized access and keep its business operations running smoothly.

### Data Loss Prevention    

We have a lot of different types of data on our networks, such as social security numbers, credit card numbers, healthcare information, and other sensitive data that needs to be protected from attackers. Data Loss Prevention (DLP) systems are intelligent solutions that help protect our data from unauthorized access.

DLP solutions can be implemented on our local workstations and devices to examine everything that is being transferred into or out of them. They can also be deployed on our network to examine all of the packets going across and ensure that no sensitive information is stored in the network traffic. For data at rest on our servers, we may want to have a DLP system running on the servers themselves.

DLP technologies can also be designed to block access to different types of hardware. For example, DLP on a workstation can allow or disallow access to data stored on a USB-connected drive. A good example of the importance of DLP associated with USB flash drives occurred in 2008, when the US Department of Defense banned the use of these devices on their network due to a worm replicating itself using USB storage.

Cloud-based DLP technologies also exist, which are able to look for predefined data strings within the traffic going through them. They can block data from going to certain URLs and prevent people from storing that data on an insecure cloud-based storage system. They can also block viruses, malware, and anything else that might traverse the network.

Email systems also need DLP to prevent sensitive information from being transferred into or out of our network. Inbound emails can be filtered with DLP using keywords or by identifying emails that are coming from imposters. Outbound DLP filtering can block anything that looks like a wire transfer or anything related to employees' personal information.

An example of the importance of DLP in email systems occurred in 2016, when an employee at Boeing emailed their spouse a spreadsheet containing the personal information of 36,000 employees. The spreadsheet was hidden in the template for a different project, and the information was transferred out over nonsecure channels. This incident underscores the importance of DLP solutions to prevent data breaches and protect sensitive information.

### Managing Security  

The location of technology is an important factor that affects the security of applications and data. From a legal perspective, companies must understand how security is impacted by differences in location, whether it's across state lines or international business. All personnel associated with the business process should have their passports in order, especially if they need to travel to another country for recovery or maintenance.

Businesses should have legal representation that is always involved in managing and controlling data and systems in their network. Geographical considerations apply to backups as well. Companies must determine where their backups are stored, whether on-site or off-site, and what type of access and control they have over the data. Companies must also manage the recovery process if they need to respond to an attack or disaster.

To prevent an attack or recover from one, companies need to document the entire process from beginning to end. The documentation process can be time-consuming, but it is essential. Companies need to identify and contain an attack as soon as possible, limiting the scope of the attack and preventing access to sensitive information.

Secure Sockets Layer (SSL) or Transport Layer Security (TLS) is used to encrypt data when sending it across a network or connecting to a third-party site. SSL inspection allows security professionals to view the encrypted data to determine if there is anything malicious inside. However, SSL inspection is not easy and requires special configuration. Companies must have trust in the certificate authorities embedded in their browsers to ensure secure connections.

There are over 170 different certificates that browsers trust, which means that for every site visited on the internet, there must be some interaction with a trusted certificate authority. The certificate authority must sign the certificate for the web server, which is the certificate the web server provides to the user.

In conclusion, the location of technology is an essential factor in securing applications and data. Companies must have legal representation that is always involved in managing and controlling data and systems in their network. Companies must also document the entire process from beginning to end to prevent or recover from an attack. SSL inspection is a useful tool for security professionals, but companies must trust the certificate authorities embedded in their browsers to ensure secure connections.

The process of intercepting and analyzing encrypted data requires a device in the middle of the communication, such as a firewall or SSL decryption device. To do this, an internal certificate authority is created and added to every user's device in the network. By intercepting the initial Hello message sent from the user, the SSL decryption device can create a proxy message to be sent to the server. After verifying the server's certificate, the device creates a new certificate for use on the internal network, allowing for the decryption, analysis, verification, and protection of inbound and outbound data.

Hashing is used for encryption, digital signatures, and other cryptographic processes by representing data as a short string of text, known as a message digest. The hashing function is irreversible, making it useful for password protection and verifying document integrity by comparing the received hash to the original. Digital signatures use the authentication and non-repudiation features of a hash to confirm the sender of a message and ensure its integrity. The use of hashing also helps maintain data security by creating different hashes for even slightly different messages, preventing attackers from accessing data by making a slight change to it.

APIs, or Application Programming Interfaces, are increasingly used for communication and control of applications in mobile devices and cloud-based technologies. APIs provide a secure and controlled way to interact with applications and allow for the creation of new applications that can access data or functionality from other applications. Overall, understanding these concepts is essential to maintaining data security and privacy.

### Site Resiliency    

Disaster recovery plans are crucial for businesses as disasters can happen anytime. Having backups and synchronizing data is important in order to be ready to put the disaster recovery plan into action at any moment. In cases where the disaster requires the data center to physically move to the disaster recovery site, the entire system needs to be shifted to the alternate location.

Sometimes businesses may have to stay in the disaster recovery location for an extended amount of time, ranging from days to even months, before moving back to the original data center. Hence, it is important to have processes and procedures in place for both getting up and running at the disaster recovery site and moving back to the original location.

Disaster recovery locations can be categorized into hot sites, cold sites, and warm sites. Hot sites are exact replicas of the production environment and have duplicate hardware, duplicate servers, and all other equipment and infrastructure. Hence, there needs to be a process in place to keep everything synchronized between the two locations. This can be achieved by updating everything in real-time between the two locations with high-speed network connections or by sending periodic updates to the disaster recovery site.

On the other hand, cold sites have none of the equipment, data, or applications currently in place. It is like an empty room with racks where businesses need to bring their own data and personnel to attend to the systems. Warm sites are a middle ground between hot sites and cold sites. They have some equipment available that can be up and running relatively quickly. The disaster recovery contract usually outlines the details of what is included in the warm site. It is important to strike a balance between the amount of equipment available and the cost required to maintain the disaster recovery location.

In conclusion, it is essential for businesses to have disaster recovery plans in place and to be aware of the different types of disaster recovery locations available. Hot sites are exact replicas of the production environment and require a process to keep everything synchronized. Cold sites are empty rooms with racks and require businesses to bring their own data and personnel. Warm sites are a middle ground between hot and cold sites and have some equipment available that can be up and running relatively quickly. It is important to strike a balance between the amount of equipment available and the cost required to maintain the disaster recovery location.

### Honeypots and Deception    

Honeypots, honeynets, honeyfiles, machine learning, and DNS sinkholes are all tools that can be used by security professionals to gather intelligence about attackers, identify their methods, and protect networks from malware and other threats.

A honeypot is a system or series of systems designed to attract attackers, with the goal of gathering information about their methods and identifying any vulnerabilities in the network. Honeypots can be created using software like Kippo, Google Hack Honeypot, or Wordpot. Honeyfiles, such as a file named passwords.txt, can be added to honeypots to act as bait for attackers. If an attacker accesses a honeyfile, an alert can be triggered to notify security teams.

Multiple honeypots can be combined to create a honeynet, which allows security teams to gather information from multiple sources and track attackers as they move from one server to another. Projecthoneypot.org is an example of a honeynet that shares information among security professionals.

Machine learning can be used to identify patterns and malicious data within large data sources. To train machine learning systems, actual malware, ransomware, viruses, and other malicious data are fed into the system. Attackers may try to deceive machine learning systems by adding fake telemetry into the data, making it more difficult for security teams to identify malware.

A DNS sinkhole is a tool that can be used to provide intelligence for security professionals. When a client requests the IP address of a fully qualified domain name (FQDN), a DNS sinkhole can respond with incorrect or invalid information. This can be used to redirect users to a specific location or to create a denial of service. However, DNS sinkholes are more commonly used to identify malware by redirecting requests to malicious sites to a machine within the organization, allowing security teams to track infected devices and prevent the spread of malware.

Intrusion prevention devices and next-generation firewalls can integrate DNS sinkholes to detect and prevent communications with known malicious sites. If a device tries to communicate with a malicious site, the DNS sinkhole can redirect the request to a known good site, triggering an alert for security teams to investigate and clean infected devices.

Overall, these tools can be used in combination to provide a comprehensive approach to network security, gathering intelligence about attackers, identifying vulnerabilities, and preventing the spread of malware and other threats.


## 2.2 – Virtualization and Cloud Computing


### Cloud Models 

Cloud service providers offer different models of cloud computing services, including Infrastructure as a Service (IaaS), Software as a Service (SaaS), Platform as a Service (PaaS), and Anything as a Service (XaaS). In the IaaS model, the cloud service provider provides hardware such as a CPU, storage, and networking connectivity, but the client is responsible for managing the operating system and applications that run on it, as well as data security. An example of an IaaS model is a web service provider that gives the client a server to run their services.

On the other hand, the SaaS model provides on-demand software that clients can use by simply logging in, without having to configure or maintain the application or data. The cloud service provider manages both the applications and the data, and the client has no responsibility for application development or maintenance. An example of a SaaS model is Google Mail.

The PaaS model is a middle ground between IaaS and SaaS, where the cloud service provider provides the client with a platform to develop their own customized applications. The provider offers the operating system, infrastructure, and virtualization services, and the client uses building blocks provided by the provider to create their own applications. Salesforce.com offers a good example of a PaaS model.

Finally, the XaaS model is a broad description of any type of service that is provided over the cloud. This could describe a set of services available on the public cloud that can be paid for as used, rather than as a large upfront cost or ongoing licensing. The concept behind XaaS is that anything that is currently done in-house with technology could potentially be outsourced into a cloud-based system. This would make IT less of an ongoing break/fix organization and more focused on applying technology needed by the organization into a cloud-based service.

In summary, cloud computing offers different models to choose from, depending on the specific needs of the organization. Cloud service providers offer IaaS, SaaS, PaaS, and XaaS, and each has its own advantages and disadvantages. Organizations must weigh the benefits and costs of each model to determine which one is most suitable for their needs.

### Edge and Fog Computing   

Cloud computing has revolutionized the way we deploy and use applications. With cloud computing, we can access a vast amount of computing power and store huge amounts of data in a cloud-based infrastructure. Cloud computing also simplifies the deployment of new applications and reduces costs significantly.

However, cloud computing also has its disadvantages. For instance, cloud applications may be hosted far from your location, causing delays in communication between you and the cloud. Additionally, there may be limited bandwidth due to wide-area network connections, and data stored in the cloud may be difficult to protect, since cloud service providers often store data in a format that may not allow for encryption.

IoT (Internet of Things) devices have rapidly increased in recent years, with devices like climate control and alarm systems running on home networks and accessible over the internet. The technology and data for these devices are primarily stored on the device itself, referred to as edge computing. With edge computing, applications and decisions made from the data collected by these applications occur on the local system and don't need to go out to the internet.

Furthermore, since the data doesn't need to be communicated to the internet, there is no need to worry about latency or wide-area network or internet connectivity. The speed and performance of the device should be at the local speed of the network, and data collected by IoT devices on the network is used to make decisions about how the devices should operate.

However, there may be times when IoT devices may provide additional functionality by taking some of the data and processing it in the cloud. Fog computing is a distributed cloud architecture that allows information to be sent into the cloud for processing without requiring that all of the data be consolidated in one single place. This means that any data an IoT device needs to make local decisions can stay local on that device.

Fog computing allows us to move data created by IoT devices into the cloud for additional processing. The data can be compared with data seen by other people, making our devices work more effectively. From a privacy perspective, this means that we can keep sensitive data on our local network and only send the information that we may feel comfortable sharing with others into the fog.

In conclusion, cloud computing and edge computing have revolutionized the way we deploy and use applications, but they also have their limitations. Fog computing is an excellent solution to bridge the gap between cloud computing and edge computing, providing real-time processing of data and optimizing or buffering information.

### Designing the Cloud 

Cloud computing has revolutionized how applications are deployed by providing access to enormous computing power and elasticity. Cloud computing also allows applications to be accessed from anywhere in the world, and to achieve this functionality, applications have to be developed and deployed in a cloud-based infrastructure.

One way to start with cloud technologies is by running systems that would normally run on a desktop on the cloud using a thin client. A thin client provides just enough computing power to connect to a desktop running on the cloud, and this type of architecture is referred to as Virtual Desktop Infrastructure (VDI) or Desktop as a Service (DaaS). The local device requires just enough power to provide users with a remote desktop view of the desktops running on the cloud.

Cloud services run on many different operating systems that execute on a single piece of hardware, which is virtualization. Virtualization involves running a hypervisor on the hardware that manages different operating systems running on the computer. A separate virtual machine may have an operating system and application, which may require additional resources for each virtual machine.

Containerization enables multiple applications to run simultaneously on a single host operating system without separate guest operating systems. Each application runs in its separate sandbox, and the containerization software, such as Docker, is used to run the separate applications. Each application is self-contained, which means everything needed to run application A is in the application A container. Containerized applications can be moved to any other system to create additional instances, and they are lightweight to deploy.

Many applications used daily are built on a single codebase, making them monolithic. Everything within the application is self-contained, including the user interface, data input and output, and business logic, all contained within a large codebase. The monolithic application creates additional complexity and makes it difficult to update a single feature. Instead, the entire codebase has to be replaced to use new features.

In conclusion, cloud computing has transformed how applications are deployed by providing access to enormous computing power, elasticity, and accessibility from anywhere in the world. Containerization provides a lightweight and efficient method of deploying multiple applications simultaneously without the need for separate guest operating systems. Monolithic applications, although useful, have limitations as they make it difficult to update a single feature without replacing the entire codebase.

### Infrastructure as Code    

In cloud computing, it is possible to deploy complex and multi-serviced applications automatically, without human intervention, through infrastructure as code. This means that the application instance is described in a series of code that can be deployed anytime. With this approach, configuration settings can be specific to individual servers, and application instances can follow a very specific configuration setting.

In addition to deploying application instances with infrastructure as code, it is also possible to use this method to manage infrastructure devices, such as Software Defined Networking (SDN). With SDN, functionality is separated into two planes of operation: the control plane and the data plane. The control plane handles the management and ongoing configuration of the device, while the data plane handles the actual operation. SDN is agile, allowing changes to be made dynamically, and can be managed from a single pane of glass.

It is important that the entire process of automated deployment follows a set of open standards, allowing for a very open and standard process regardless of the underlying infrastructure. An example of deploying security devices using SDN is deploying an internal firewall that connects multiple web servers and a database server, while being able to manage the flows of traffic between those devices.

As cloud-based architecture is constantly in motion, it is important to be able to monitor and understand the traffic flows of different application instances. This is where Software Defined Visibility (SDV) comes in. SDV allows for the deployment of security devices while being able to understand what type of data is flowing between different systems. This includes understanding the Virtual Extensible LAN (VXLAN) and what type of data might be encrypted using SSL or TLS.

With SDV, it is possible to get real-time views of web usage, host name usage, and the top applications in use based on what these devices are seeing on the network. And if a potential threat is identified, APIs can be used to control what these application devices may be sending across the network.

In summary, infrastructure as code and SDN are key components of cloud computing that allow for the automated deployment of complex and multi-serviced applications. SDV is also important for monitoring and understanding the traffic flows of different application instances, as well as deploying security devices. By following open standards and understanding the underlying infrastructure, these technologies can help make cloud computing more efficient and secure.

### Virtualization Security    

Cloud computing offers numerous benefits, including the ability to deploy new application instances quickly and easily. With the click of a button, multiple servers, databases, and security devices can be deployed automatically. However, while this is a great feature, it also creates a problem known as virtual machine sprawl.

Virtual machine sprawl occurs when virtual machines are continuously built on a network, but are not deprovisioned when they are no longer needed. This can lead to confusion and difficulty in identifying which virtual machines are related to which application instances, making it harder to remove resources from the network. To prevent this problem, a formal process for provisioning and deprovisioning application instances should be put in place.

It's also essential to identify every virtual object and track it from creation to deprovisioning. Although virtual machines are self-contained, there is a risk of a virtual machine escape, where someone on one virtual machine gains access to resources on another virtual machine. This can lead to significant security concerns, as someone with access to this type of exploit would effectively have full control of your virtual environment, applications, and data.

Virtual machine escapes are rare, but they do happen. In March 2017, at the Pwn2Own competition, a hacking contest, an attacker was able to take advantage of a JavaScript engine bug in Microsoft Edge Browser. This bug allowed code execution within the Microsoft Edge sandbox, which then allowed access to the Windows 10 kernel. The attacker then used a Windows 10 kernel bug to compromise the guest operating system of the virtual machine, before exploiting a hardware simulation bug in VMware and escaping to a separate VM host on that service.

To prevent virtual machine escapes, it's important to have a secure virtual environment, with regular updates and patches for the operating system and browsers. Additionally, access controls should be put in place to limit the ability of users to interact with other virtual machines. By following these guidelines, it's possible to enjoy the benefits of cloud computing while minimizing the risks.


## 2.3 – Secure Application Development


### Secure Deployments    

In software development, getting an application from the development stage into the production environment is a crucial process that involves several checks to ensure that everything runs smoothly. In rolling out updates or patching existing operating systems, there is also the challenge of testing and deploying these patches into the system. In this article, we will discuss how the use of sandboxing can help in the application development process, the different phases of software development, and the steps involved in deploying an application in a production environment.

In software development, sandboxing refers to an isolated testing environment that allows developers to test different aspects of the application without affecting other parts of the production environment. The developer can test their code, try different ideas and concepts, and see what the results might be. Once the developer has written the code in a secure environment and tested different aspects of its functionality, they move on to the testing environment, where they bring all the code together to see how all the different parts will react to each other. This phase allows the developers to check if the features and functions of the application are working as expected.

After the application developers are satisfied that the application is working as expected, they can hand it off to the quality assurance (QA) team. The QA team tests the application to ensure that it's working as expected, including testing new features and ensuring that previously fixed bugs are still corrected in the new version. Once the QA team has completed their tests and is satisfied that the software is ready to be released, the next step is the staging environment. Here, a copy of the production data is taken and copied over to be used in a real-world environment that tests all the capabilities that the production environment requires. This phase allows for a final test of the data and performance of the application in a production environment.

After completing all the previous tests, it's time to deploy the application into production. This process may involve implementing new servers, installing additional software, and restarting or interrupting service for existing production users. Before deploying the application into production, it's crucial to define all the important security characteristics and baseline required to secure and maintain the security of the application. We need to check firewall settings, patch levels of the application and the operating system, and make sure that the operating system files are up to date with the latest security patches.

In conclusion, deploying an application from the development stage to the production environment is a crucial process that involves several checks to ensure that everything runs smoothly. The use of sandboxing in the application development process allows developers to test different aspects of the application without affecting other parts of the production environment. The different phases of software development include the developer's phase, testing phase, quality assurance phase, staging environment phase, and the deployment phase. The steps involved in deploying an application in a production environment include defining all the important security characteristics, checking firewall settings, patch levels, and ensuring that the operating system files are up to date with the latest security patches.In software development, getting an application from the development stage into the production environment is a crucial process that involves several checks to ensure that everything runs smoothly. In rolling out updates or patching existing operating systems, there is also the challenge of testing and deploying these patches into the system. In this article, we will discuss how the use of sandboxing can help in the application development process, the different phases of software development, and the steps involved in deploying an application in a production environment.

In software development, sandboxing refers to an isolated testing environment that allows developers to test different aspects of the application without affecting other parts of the production environment. The developer can test their code, try different ideas and concepts, and see what the results might be. Once the developer has written the code in a secure environment and tested different aspects of its functionality, they move on to the testing environment, where they bring all the code together to see how all the different parts will react to each other. This phase allows the developers to check if the features and functions of the application are working as expected.

After the application developers are satisfied that the application is working as expected, they can hand it off to the quality assurance (QA) team. The QA team tests the application to ensure that it's working as expected, including testing new features and ensuring that previously fixed bugs are still corrected in the new version. Once the QA team has completed their tests and is satisfied that the software is ready to be released, the next step is the staging environment. Here, a copy of the production data is taken and copied over to be used in a real-world environment that tests all the capabilities that the production environment requires. This phase allows for a final test of the data and performance of the application in a production environment.

After completing all the previous tests, it's time to deploy the application into production. This process may involve implementing new servers, installing additional software, and restarting or interrupting service for existing production users. Before deploying the application into production, it's crucial to define all the important security characteristics and baseline required to secure and maintain the security of the application. We need to check firewall settings, patch levels of the application and the operating system, and make sure that the operating system files are up to date with the latest security patches.

In conclusion, deploying an application from the development stage to the production environment is a crucial process that involves several checks to ensure that everything runs smoothly. The use of sandboxing in the application development process allows developers to test different aspects of the application without affecting other parts of the production environment. The different phases of software development include the developer's phase, testing phase, quality assurance phase, staging environment phase, and the deployment phase. The steps involved in deploying an application in a production environment include defining all the important security characteristics, checking firewall settings, patch levels, and ensuring that the operating system files are up to date with the latest security patches.

### Provisioning and Deprovisioning    

Provisioning is the process of making an application available, and it involves deploying various components, such as web servers, database servers, middleware servers, and configurations. These components must work together to create an application instance. When deploying an application, it's crucial to consider security components to ensure that the application and operating system have the latest security patches, the network architecture is secure, and there are no malicious binaries in the software. Furthermore, the security posture of the workstation must be checked to ensure that it's updated with the latest patches and configurations.

The workload associated with the provisioned application may vary, depending on several criteria. As such, it's essential to build an application instance with a particular level of scalability that can handle a specific number of transactions per second. If the application becomes more popular and requires more resources, such as an increase in transactions per second, multiple application instances can be deployed. This ability to increase or decrease the available resources for an application based on its workload is referred to as elasticity.

Orchestration is another key element in cloud computing that enables the automation of the provisioning and deprovisioning of applications. With orchestration, it's possible to automate the deployment of servers, network configurations, and security components associated with the application. The orchestration process can also be used to determine the geographic location of the application instance based on user availability.

The automation of orchestration applies to all aspects of the application, including security components, which must be removed during the deprovisioning process. This process involves removing all remnants of the application from the environment and removing individual rules in existing firewalls.

In conclusion, provisioning an application involves deploying various components that must work together to create an application instance. Security components must also be considered to ensure that the application and operating system have the latest security patches, the network architecture is secure, and there are no malicious binaries in the software. The application instance must be built with a particular level of scalability to handle the workload associated with it, and orchestration can be used to automate the deployment of servers, network configurations, and security components. Finally, the deprovisioning process involves removing all remnants of the application from the environment, including security components, and deciding what to do with the data created while the application was in place.

### Secure Coding Techniques 

When creating an application, developers must balance the time it takes to create the app with the quality of the final product. An application needs to be able to perform all necessary functions, but it also needs to be secure. The QA process takes a lot of time as developers test and verify different features of the application. Security vulnerabilities must be patched before someone exploits them. One way to make an application more secure is to create stored procedures for database calls. Instead of having the client send information to the database server to run the query, the query is stored on the database server, and the application sends a message saying to run a stored procedure. Obfuscation is another method that developers use to make their code more secure. Obfuscated code is difficult for humans to read, but the computer understands it perfectly. Reusing code between different applications can cause a security vulnerability to spread into all the other apps that use it. Developers must ensure that the code performs a useful function and avoids code reuse or dead code. Attackers can identify vulnerabilities in an application by finding a place where the input is not properly validated. For example, if an application is asking for a zip code, it should only expect a certain set of numbers over a certain range. The application should check all data to ensure it is in the right format and make any necessary corrections. This is known as normalization.

Creating an application requires careful consideration of many factors, including the time it takes to create the app, the quality of the final product, and security. Developers must balance these factors to create an application that performs all necessary functions while also being secure. The QA process takes time, but it is necessary to test and verify different features of the application. Security vulnerabilities must be patched before someone exploits them. Stored procedures can make an application more secure by storing queries on the database server instead of having the client send information to the database server to run the query. Obfuscation is another method that developers use to make their code more secure. Obfuscated code is difficult for humans to read, but the computer understands it perfectly.

However, reusing code between different applications can cause a security vulnerability to spread into all the other apps that use it. Developers must ensure that the code performs a useful function and avoids code reuse or dead code. Attackers can identify vulnerabilities in an application by finding a place where the input is not properly validated. Normalization is necessary to ensure that all data is in the right format and to make any necessary corrections.

For example, a zip code should only be a certain number of characters long, and it should usually consist of a series of numbers. There are multiple places for data input, such as uploading files or putting information into a form. The application should check all data to ensure it is in the right format and make any necessary corrections.

In conclusion, creating a secure application requires developers to consider many factors, including time, quality, and security. Stored procedures and obfuscation can make an application more secure, but developers must also avoid code reuse and dead code. Attackers can identify vulnerabilities in an application by finding a place where the input is not properly validated. Normalization is necessary to ensure that all data is in the right format and to make any necessary corrections. By considering all of these factors, developers can create a secure application that performs all necessary functions.

### Software Diversity    

Cyber attackers are experts in identifying vulnerabilities in software applications and exploiting them wherever they find them. To counter this, constant patch updates are necessary for both operating systems and applications. However, a unique approach to this problem is to create software that is different on every workstation, even if everyone is running the same version of the software. This technique is known as software diversity.

Software diversity involves using different tricks in the compiler during the compilation process to change where the paths go, resulting in different binary files for the same application. This does not alter the functionality of the application, only the final binary file. If an attacker finds a vulnerability in a binary file on a person’s machine and creates an exploit for it, they may not be able to use that exploit on another person’s machine because it is running a different version of the file. This would minimize the attack surface, limiting any potential attack to a specific type of binary file.

Although this approach requires additional effort to create and deploy different binary files, it can significantly enhance security by limiting the scope of potential attacks. It is important to note that this technique should not be the only security measure in place, as attackers can still find other ways to exploit vulnerabilities. Regular security updates, training employees to recognize and report potential security threats, and implementing additional security measures are still necessary. However, software diversity can be an effective tool in reducing the attack surface and protecting against cyber threats.

### Automation and Scripting    

The application development process is always changing, and it’s important to keep up with these changes to maintain security. Automation can be used to help deploy and monitor applications. For example, monitoring the storage area of log files is important to ensure that it doesn’t fill up and cause the application to fail. This is especially important for cloud-based technologies that may be automatically provisioned and deprovisioned.

Automation is crucial in Continuous Integration (CI), where developers may update and merge an application many times a day, which could potentially create security issues. Automated security checks need to be in place to evaluate the updated code against existing security baselines. Once the software is out of the development process, more extensive security checks are made to ensure the application is as safe as possible when deployed to production.

Continuous delivery involves automating the testing and release of the application, with automated security checks occurring during the testing process. In this case, the application is not deployed until a human clicks a button to deploy it to production. With continuous deployment, the testing process and deployment are entirely automated, with no need for human intervention. If all automated security checks pass, the application is automatically deployed to production.

Automating the entire process is beneficial in many ways. It ensures that the process is consistent, and human error is minimized. It also frees up developers to focus on developing the application rather than worrying about deploying and monitoring it.

Automating security checks during the development process, testing, and deployment is crucial for maintaining application security. By automating these checks, potential security issues can be identified and resolved before they can cause damage.



## 2.4 – Authentication and Authorization
Authentication Methods 
Biometrics    
Multi-factor Authentication    
## 2.5 – Resilience
Disk Redundancy    
Network Redundancy    
Power Redundancy    
Replication    
Backup Types 
Resiliency    
## 2.6 – Embedded Systems
Embedded Systems  
Embedded Systems Communication    
Embedded Systems Constraints    
## 2.7 – Physical Security Controls
Physical Security Controls 
Secure Areas    
Secure Data Destruction    
## 2.8 – Cryptographic Concepts
Cryptography Concepts    
Symmetric and Asymmetric Cryptography    
Hashing and Digital Signatures 
Cryptographic Keys    
Steganography    
Quantum Computing    
Stream and Block Ciphers    
Blockchain Technology    
Cryptography Use Cases    
Cryptography Limitations    
# Section 3 – Implementation


## 3.1 – Secure Protocols

### Secure Protocols 

The use of secure protocols and encryption is essential for ensuring secure communications on the internet. The Real-Time Transport Protocol (RTP) is commonly used for Voice over IP (VoIP) and is now available in a secure version called the Secure Real-Time Transport Protocol (SRTP) which uses Advanced Encryption Standard (AES) for encryption along with additional security features like authentication, integrity, and replay protection. Legacy protocols like the Network Time Protocol (NTP) were never designed with any security features, which has led to attackers taking advantage of them to perform distributed denial of service attacks. To address this, the NTPsec protocol has added a number of security features to NTP and cleaned up old code to remove vulnerabilities.

One way to secure email communications is by using the Secure Multipurpose Internet Mail Extensions (SMIME) which is a public-private key encryption mechanism that allows encryption and digital signatures for integrity. Public key infrastructure (PKI) is necessary to manage these keys. Other email protocols like Post Office Protocol version 3 (POP3) and Internet Message Access Protocol (IMAP) have security extensions such as StartTLS and Secure IMAP that use Secure Sockets Layer (SSL) to provide confidentiality. SSL is now commonly referred to as Transport Layer Security (TLS) which is the backbone of most encryption on the internet. HTTPS (HTTP over SSL/TLS) is the most common form of SSL/TLS, using public key encryption to transfer symmetric keys across the network for use during communication.

To securely transfer data between two locations over the internet, an encrypted tunnel is necessary. One of the most common types of encrypted tunnel is the Internet Protocol Security (IPsec) which allows encryption of data over the public internet while providing packet signing for integrity and anti-replay features. IPsec is standardized and well-established, allowing different manufacturers' equipment on both ends of the tunnel to communicate with each other.

When transferring files between devices, secure protocols like FTPS and SFTP should be used. FTPS is the File Transfer Protocol Secure which uses SSL to encrypt information while SFTP is the SSH File Transfer Protocol which uses SSH to provide encryption along with additional management capabilities.

Most enterprise networks have a centralized directory where information is stored, accessed using the Lightweight Directory Access Protocol (LDAP). The original standard for this directory was DAP (Directory Access Protocol), which ran on the OSI protocol stack. When this was updated for TCP/IP networks, the LDAP version was created.

In conclusion, secure protocols and encryption are essential for ensuring secure communications on the internet. SRTP, NTPsec, SMIME, StartTLS, Secure IMAP, HTTPS, IPsec, FTPS, SFTP, and LDAP are some of the secure protocols and encryption methods used to ensure confidentiality, integrity, and authenticity of data during communication.

## 3.2 – Host and Application Security
### Endpoint Protection    

As we store an increasing amount of sensitive information on our computing devices and use a variety of devices, we must ensure the security of each device and platform. In this regard, Layered Protection Defense in depth is used to add security features to our endpoint devices. Antivirus and anti-malware software are two of the most commonly used security features to stop viruses, worms, trojan horses, and other malicious software attacks. However, signature-based detection can be bypassed by attackers, so endpoint detection and response (EDR) mechanisms are used to find malicious software through machine learning and process monitoring. DLP or data loss prevention is used to prevent sensitive information from being sent across the network, and it prevents this information from being sent across the network in the clear or even encrypted. NGFWs or next-generation firewalls can identify the applications that are flowing across the network, regardless of the IP address or port number that might be in use. NGFWs can not only identify the applications running over the network but also identify individual features within the application. Thus, security policies can be set to allow or disallow access to those applications on the network.

With the use of different computing devices and platforms, we must ensure that each one is protected. Layered Protection Defense in depth is a technique to protect our endpoint devices by adding security features to each one of them. Antivirus and anti-malware software are two of the most commonly used security features. These software are designed to stop viruses, worms, trojan horses, and other malicious software attacks. However, attackers have found ways around signature-based detection, which is used in antivirus and anti-malware software. To combat this, endpoint detection and response (EDR) mechanisms are used to find malicious software through machine learning and process monitoring. EDR can identify malicious types of actions on our computer and block them rather than blocking a signature. EDR can perform a root cause analysis to determine why a particular behavior occurred and can isolate the system from the rest of the network. It can also quarantine the malicious software into a different part of the operating system or remove what's on the system now and roll back to a previously known good configuration. This process can be automated using application programming interfaces (APIs), which means that the identification, the removal, and the restoration of this system can be done without the need to involve any individual technician.

Organizations store databases of very sensitive information, including medical records, social security numbers, credit card numbers, and other types of confidential data. To prevent this sensitive data from being sent across the network in the clear or even encrypted, data loss prevention (DLP) mechanisms are used to stop data leakage. DLP involves many different systems, such as a DLP solution based in a firewall, on each individual system, or based in the cloud to examine all the emails going in or out of an organization. DLP can identify sensitive data within any of these data streams and block that information from being transferred outside of your private network.

Traditionally, we have used firewalls to allow or block traffic based on an IP address and a port number. However, with the need for more security, we now require more granular scale security. This is where next-generation firewalls (NGFWs) come in. NGFWs can identify the applications that are flowing across the network, regardless of the IP address or port number that might be in use. With NGFWs, you can set security policies to allow or disallow access to those applications on the network. NGFWs can not only identify the applications running over the network but also identify individual features within the application. This means you can set security policies that would allow someone to view the information on Twitter but prevent them from posting any information on Twitter. Most NG

### Boot Integrity    

In the world of IT security, attackers are constantly looking for ways to compromise the security of systems, operating systems, and data that are being kept on those operating systems. They try to exploit the boot process of an operating system to gain control over it, embedding malicious software that has full control of the system, making it difficult to be removed. To ensure the protection of operating systems, security measures like Secure Boot, Trusted Boot, and Measured Boot have been put in place, all of which fall under the Chain of Trust. This ensures that every step in the boot process is protected, creating a hardware root of trust that assures the system is safe and secure.

To secure a system, users rely on a level of trust with the operating systems and software that they are using. They put security controls in place to be sure that the systems are safe. For individual systems, users rely on Trusted Platform Modules (TPM). TPMs are pieces of hardware that are used for cryptographic functions used by applications within an operating system. It has a cryptographic processor commonly used as a random number generator or key generator. It also stores keys that can be burned in and not changed. This ensures that it is unique to every computer that has a TPM. TPMs have memory that stores information, such as encryption keys or configuration information about the hardware where it is installed. The information within the TPM is password-protected and comes with an anti-brute force technology built into it.

The BIOS provides software security, with the UEFI BIOS having a function called Secure Boot. This feature checks the bootloader of the operating system to ensure that no malicious software has altered it. The bootloader's digital signature verifies with the operating system manufacturer's digital signature, ensuring that no malicious software has changed any part of that bootloader. There is a trusted certificate that the bootloader must be signed by, and that certificate is compared to the digital signature that is in the bootloader. The operating system's bootloader must be signed by a certificate that is trusted, or it has to be a manually approved digital signature, so that users know when they are starting the operating system, no part of that bootloader has been changed by any malicious software.

Once the Secure Boot process is complete, the system moves to the Trusted Boot process. During this process, the bootloader, which has not been changed, verifies the digital signature of the operating system kernel. The kernel is the part of the operating system that communicates between the hardware and software, ensuring that the software can operate the hardware correctly. This verification ensures that no malicious software has changed the kernel. If the kernel has not been altered, the bootloader then loads the operating system.

Finally, the system moves to the Measured Boot process, which generates a hash of each component of the operating system, including the bootloader and kernel. A hash is a unique value that represents the information stored in the operating system. If any component of the operating system has been altered, it generates a different hash. The system compares the generated hash to a reference hash to determine whether the component has been altered. If the hash is the same, the component is loaded.

These security measures help to ensure the safety and security of operating systems. They provide a hardware root of trust that ensures that no malicious software can be installed without the user's knowledge. It also assures that the bootloader, kernel, and components of the operating system have not been changed. These measures protect users' data and ensure that they can trust the websites they visit. The security measures are constantly evolving, ensuring that users' systems are always protected.

### Database Security    

Data security is a critical aspect of database management since the data stored in databases are often highly valuable. As a result, it is essential to protect the information stored in a database as well as the data transmitted to and from it. Compliance rules such as PCI DSS, HIPAA, and GDPR necessitate the management of data in a database.

The primary goal of database security is to ensure that data is always available, which guarantees that businesses can always operate. Database breaches can be highly disruptive and costly to repair. To protect data in a database, one strategy is to use tokenization, a technique that replaces sensitive data with tokens that have no representation of the original value. For example, a social security number could be stored in a database as a token that is unrelated to the original value.

Tokenization is commonly used in the storage of credit card numbers. To protect credit card information from unauthorized access, temporary credit card numbers or tokens are used during transactions, and different tokens are used for subsequent purchases. Limiting the use of tokens is a valuable part of the tokenization process since it prevents attackers from using a token for multiple purchases.

Tokenization does not involve encrypting or hashing sensitive data, which eliminates the need for any cryptographic function. Rather, it involves replacing sensitive data with a tokenized value that is used for transactions.

Hashing is another approach to store data in a database. This method is often used with passwords, where passwords are stored as a message digest or a fixed length string of text. By storing the hash instead of the password, attackers will not be able to access the original password even if they gain access to the data. Hashes have unique characteristics that make them useful in data storage. For example, different inputs will have different hash values, and there will not be any duplicated hash values.

Hashes are a one-way trip, meaning it is not possible to reverse the process and determine the password based on the hash stored in the database. The login process involves hashing the login information and comparing it to the hashed value stored in the database. Using a different randomized salt with each user's password adds more randomization to the hash, making it more challenging for attackers to determine the original password.

Rainbow tables are pre-computed sets of hashes and original values. Adding salt to the hashing process makes it more challenging to use rainbow tables to determine the original password since the rainbow table would have to perform a brute force operation to ascertain the password. Brute force is a slow and time-consuming process that would take longer than using a rainbow table.

In conclusion, database security is vital since the data stored in databases are often highly valuable. Protecting this data using techniques such as tokenization and hashing, as well as complying with rules and regulations, ensures that businesses can always operate. Using a randomized salt with each user's password adds another layer of security to the hash, making it more difficult for attackers to determine the original password. These techniques ensure that sensitive data stored in databases remains secure and that businesses can operate without disruption.

### Application Security 

As security professionals, it is essential to ensure that our applications and operating systems are always updated to the latest version and free from bugs. However, the process of securing an application starts with the developers themselves, who are tasked with building secure applications. While the developers aim to create applications that are functional and secure, the primary goal is often to make sure the application works to specification. Quality Assurance (QA) teams are responsible for testing the application to ensure it is secure, but vulnerabilities may go unnoticed, and attackers may exploit them.

To prevent attacks, developers must validate the type of input that goes into the application. This process of checking and correcting the data that is being input is called normalization. For example, a zip code may be defined as only having a certain number of characters with a letter in a specific column. If the data entered into the application does not conform to the defined rules, it should be rejected or corrected. It is essential that application developers understand the type of input used and how the application handles it. Attackers may use third-party tools, such as fuzzers, to randomize input into the application, attempting to make the application perform unexpectedly or replicate an error.

Fuzzing refers to a dynamic analysis task where random data is injected into the application's input. Attackers are looking for anything out of the ordinary that may cause a buffer overflow or application crash, providing them with an opportunity to exploit a vulnerability. There are many types of fuzzing tests, and they take a lot of time and processing power, which is why they are automated. Carnegie Mellon's Computer Emergency Response Team (CERT) released a fuzzer called the CERT Basic Fuzzing Framework (BFF), which can be downloaded and used on your machine. Cookies are another security concern. They are pieces of information stored on a computer from a browser and are commonly used to keep track of information used for a limited amount of time. Cookies should be protected, and secure cookies have attributes marked as "secure" to protect them.

In conclusion, developers must ensure that the applications they create are secure by validating the input data, and QA teams must test them to make sure they are free from vulnerabilities. Fuzzing is a critical component of dynamic analysis, and fuzzing tests take a lot of time and processing power. Carnegie Mellon's CERT Basic Fuzzing Framework can be downloaded and used on your machine for testing purposes. Finally, cookies are pieces of information stored on a computer from a browser, and secure cookies should be protected to prevent unauthorized access.

### Application Hardening    

Application hardening is a process that involves minimizing attack surfaces and limiting the ability of an attacker to exploit an application. As a security professional, there are several techniques that can be employed to ensure that applications are as secure as possible. This article discusses some of these techniques.

Compliance mandates are one way to ensure application hardening. For example, there are specific regulations regarding HIPAA servers or PCI-DSS credit card protection, which provide guidance on how to harden applications. The Center for Internet Security (CIS), the Network and Security Institute (SANS), and the National Institute of Standards and Technology (NIST) are some of the resources that provide guidance on application hardening.

Limiting what ports may be accessible on a device is another technique that can be used to harden applications. This is commonly done with a firewall, where IP addresses and port numbers are restricted, and in some cases, next-generation firewalls can limit the applications that can flow over a particular IP address and port number. It's not unusual to find applications or services running on a device that have opened up port numbers that are accessible from the network. This is something that can commonly happen if you install software, or the default configuration for an operating system. To limit access to only necessary ports, it's common to perform an Nmap scan and verify what ports happen to be open currently.

For Windows administrators, several application hardening tasks can be performed inside of the Windows Registry, which contains configuration settings for the Windows operating system and the applications that run on that operating system. It's common to use third-party tools that can show the registry settings before an application has been installed and what settings have changed after the application has been installed. Some of these registry settings are essential from a security perspective, as the registry can allow someone to configure permissions and other applications to make changes to the registry. In some cases, vulnerabilities can be disabled in the registry.

To prevent third-party access to the data that we store on our computers, hard drives and storage devices can be used that will encrypt the information that is stored. This is sometimes handled in the file system itself, through something called full disk encryption. Windows BitLocker is an FDE utility built into the Windows operating system. In some highly secure environments, a type of encryption on a storage drive that's built into the hardware of the drive itself is used.

Every operating system requires a different set of techniques to make it more hardened. For example, there is Windows, Linux, iOS, Android, and other operating systems, and every single operating system is going to have a different set of techniques to be able to make that operating system more hardened. One common technique across all operating systems is to always keep the operating system up to date with the latest versions. These can be updates to the core operating system itself, deployed using service packs, or individual security patches that are installed one by one.

Finally, it's essential to harden user accounts. It's critical to ensure that all users have very good passwords, and there is a password policy for every single user. The accounts that people use to log in should have limitations that only allow them to perform the tasks that are required for their job function.

In conclusion, there are several techniques that security professionals can use to ensure application hardening. Compliance mandates, limiting accessible ports, using the Windows Registry, encrypting data, and keeping operating systems up to date are some of the techniques discussed in this article. Additionally, hardening user accounts and ensuring password policies are in place are essential steps in the process of application hardening.

## 3.3 – Secure Network Designs
### Load Balancing    

Load balancing is a technique that distributes incoming traffic across multiple devices, rather than having a single server in place. Multiple web servers work behind the scenes, and when a user accesses a website, their query is distributed to one of the available servers without the end user's knowledge. This technique is useful for large implementations, and many of the world's largest networks use load balancing for their web servers, database servers, and other infrastructure services. If one of the servers fails, the load balancer recognizes the failure and continues to use the remaining servers.

The primary function of a load balancer is to balance the load across multiple servers, and you can configure the load balancer to manage that load across several servers. You can also offload some of the TCP overhead onto the load balancer, which maintains the speed of the communication between the load balancer and the servers. Additionally, the load balancer might perform SSL encryption and decryption in the hardware, rather than on individual servers, which might have additional CPU cycles.

A load balancer can provide caching services and keep a copy of frequently accessed responses. If a user requests one of these frequently accessed responses and the load balancer already has it in the cache, it can reply back to the user without accessing any of the local servers. Load balancers can also provide "quality of service" functionality, which prioritizes certain applications over others. Additionally, load balancers might switch certain applications to individual servers, while other applications might switch to other servers within the same load balancer.

The operation of a load balancer can be configured in many ways, such as in a round-robin form. This ensures that all the servers get an equal amount of load across everyone communicating with the network. Variants to the round-robin process exist, such as weighted round-robin, which prioritizes one server over another, and dynamic round-robin, which sends the next request to the server that has the lightest load.

Load balancing is also a staple for active/active server load balancing, where all the servers are active simultaneously. If one server fails, the others can pick up the load and continue to operate without any disruption to the user. In some instances, a user must always communicate with the same server. In those cases, a load balancer must support affinity, which means that a user will always be distributed to the same server.

Affinity is usually tracked using a session ID or a combination of variables, such as an IP address and a set of port numbers. If the same IP address and port number combination is in use, then the communication will always go to the same server. Load balancers can also be set up in an active/passive mode, where some servers are actively in use, and others are on standby mode. If one active server fails, another device can move into active mode and begin providing services through the load balancer.

In conclusion, load balancing is a technique that distributes incoming traffic across multiple devices to ensure that resources are available to more people than having a single server in place. Load balancers can balance loads across multiple servers, offload TCP overhead, and perform SSL encryption and decryption, among other things. They also provide caching services, quality of service functionality, and content switching, among others. Load balancers can be configured in many ways, such as round-robin, weighted round-robin, and dynamic round-robin. They also support affinity and active/passive modes, ensuring that users receive seamless service, even if a server fails.

### Network Segmentation    

IT security is all about segmentation, which is the process of allowing or disallowing traffic between different devices. There are several ways to achieve segmentation, including physical segmentation between devices, logical separation within the same device, and virtual segmentation with virtual systems. Segmenting application instances into separate private segments is sometimes common, especially for applications with high bandwidth requirements.

Security can also be set up through segmentation. For example, database servers that contain sensitive information can be segmented from users so that they cannot talk directly to those servers. Segmentation may also be done for legal or regulatory reasons. For example, PCI compliance requires that segmentation be mandated to prevent any type of user access to credit card information.

One way to segment a network is to have physically separate devices. For example, one switch may contain all web servers, while the other switch may contain all database servers. This ensures that web servers never accidentally communicate with a database server, and the database servers have no access to the web servers. Similarly, customer data can be kept separate by keeping customer A on one switch and customer B on another. While physical segmentation is effective, it can be expensive as it requires separate maintenance, upgrades, and power supplies for each device.

Logical segmentation, achieved through Virtual Local Area Networks (VLANs), can achieve the same functionality without the need for separate devices. For example, customers can be separated into different VLANs, preventing them from communicating directly with each other. As with physical segmentation, direct communication between separate VLANs requires a cable or a third-party device like a router.

Another way to separate internal and external traffic is to build a completely separate network just for incoming traffic, sometimes called a screened subnet or Demilitarized Zone (DMZ). A firewall redirects incoming traffic to the screened subnet, where users can access the services on the network without having access to the internal network. This provides an extra layer of security to protect against unauthorized access.

An extranet is a similar design to a screened subnet, but it has additional authentication requirements. It is designed for vendors, suppliers, and other partners who need access to internal resources. Users gain access through an authentication process or login screen.

Finally, an intranet is a network that is only accessible from inside the organization. It is designed for internal communication and access to resources from headquarters or remote sites. Intranets provide an additional layer of security as they are not accessible from outside the organization.

In conclusion, segmentation is a critical component of IT security. There are several ways to achieve segmentation, including physical separation, logical separation, and virtual segmentation. Segmentation can be used to ensure that sensitive data is protected, prevent accidental communication between devices, and comply with legal and regulatory requirements. Different types of segmentation, such as screened subnets and extranets, can be used to protect against unauthorized access from external sources, while intranets provide an additional layer of security for internal communication and access to resources.
 
### Virtual Private Networks 
### Port Security 
### Secure Networking  
### Firewalls 
### Network Access Control    
### Proxy Servers    
### Intrusion Prevention    
### Other Network Appliances    
## 3.4 – Wireless Security
### Wireless Cryptography    
### Wireless Authentication Methods    
### Wireless Authentication Protocols    
### Installing Wireless Networks    
## 3.5 – Mobile Security
### Mobile Networks    
### Mobile Device Management  
### Mobile Device Security    
### Mobile Device Enforcement 
### Mobile Deployment Models    
## 3.6 – Cloud Security
### Cloud Security Controls    
### Securing Cloud Storage    
### Securing Cloud Networks    
### Securing Compute Clouds    
### Cloud Security Solutions    
## 3.7 – Identity and Account Management
### Identity Controls    
### Account Types    
### Account Policies    
## 3.8 – Authentication and Authorization Services
Authentication Management    
PAP and CHAP    
Identity and Access Services    
Federated Identities    
Access Control  
## 3.9 – Public Key Infrastructure
Public Key Infrastructure 
Certificates    
Certificate Formats    
Certificate Concepts    
# Section 4 – Operations and Incident Response

## 4.1 – Security Tools
Reconnaissance Tools – Part 1 
Reconnaissance Tools – Part 2  
File Manipulation Tools 
Shell and Script Environments    
Packet Tools    
Forensic Tools    
## 4.2 – Incident Response
Incident Response Process  
Incident Response Planning  
Attack Frameworks    
## 4.3 – Investigations
Vulnerability Scan Output    
SIEM Dashboards    
Log Files    
Log Management    
## 4.4 – Securing an Environment
Endpoint Security Configuration    
Security Configurations    
## 4.5 – Digital Forensics
Digital Forensics    
Forensics Data Acquisition  
On-Premises vs. Cloud Forensics    
Managing Evidence    
# Section 5 – Governance, Risk, and Compliance

## 5.1 – Security Controls
Security Controls    
## 5.2 – Regulations, Standards, and Frameworks
Security Regulations and Standards    
Security Frameworks    
Secure Configurations    
## 5.3 – Organizational Security Policies
Personnel Security  
Third-party Risk Management    
Managing Data    
Credential Policies    
Organizational Policies    
## 5.4 – Risk Management
Risk Management Types    
Risk Analysis  
Business Impact Analysis    
## 5.5 – Data Privacy
Privacy and Data Breaches    
Data Classifications    
Enhancing Privacy    
Data Roles and Responsibilities    
